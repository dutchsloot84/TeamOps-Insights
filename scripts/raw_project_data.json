{"items":[{"content":{"body":"The initial Jira webhook pipeline was introduced in **PR #80** with DynamoDB-backed storage. This Epic extends that work by making the pipeline production-grade: adding schema validation, idempotency, metrics, and developer documentation.\n\n### Acceptance Criteria\n- Webhook receiver (API Gateway + Lambda) remains deployed via CDK.\n- Signature/HMAC validation enforced for all incoming events.\n- Payloads normalized into schema `{issue_key, fields, changelog, updated_at, version}`.\n- Idempotent writes into DynamoDB (dedup by issue ID + updated timestamp).\n- Structured logging and retries re-use patterns from **PR #79**.\n- Metrics (success/failure counts, dedup events) published to CloudWatch.\n- Documentation updated to include webhook setup and troubleshooting.\n\n### Codex Prompt\nYou are a senior DevOps engineer and Python developer.  \n\n**Issue Reference: Epic: Jira Webhook-based Sync**  \n\nEnhance the Jira webhook ingestion pipeline introduced in PR #80.  \n\n**Core Tasks**\n1. Extend existing Lambda handler:\n   - Validate HMAC signatures (use shared secret from Secrets Manager).\n   - Enforce normalized schema and idempotent writes into DynamoDB.\n2. Add structured logging (building on #79).\n3. Add CloudWatch metrics for ingestion (processed, deduped, failed).\n4. Update CDK stack to enforce least-privilege IAM for DynamoDB, Secrets, and Logs.\n5. Write integration tests using sample webhook payloads.\n\n**Acceptance Criteria**\n- Valid Jira webhooks ingested into DynamoDB consistently.\n- Duplicate webhook deliveries do not produce duplicates.\n- CloudWatch metrics visible for success/failure.\n- Tests pass with recorded sample events.","number":88,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Jira Webhook-based Sync (Epic)","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/88"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfLsqg","labels":["cli","jira","infra"],"milestone":{"description":"Graph + Multimodal RAG + Project Memory","dueOn":"","title":"Release Copilot v2.0"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Backlog","title":"Jira Webhook-based Sync (Epic)"},{"content":{"body":"The Jira webhook ingestion pipeline (introduced in PR #80) writes issue events into a DynamoDB-backed cache.  \nHowever, webhooks only capture **future** changes. Since no audits have been run yet, the cache must be **seeded with a baseline set of Jira issues** before the first audit can succeed.  \n\nThis one-time bootstrap job will use Jira JQL queries to fetch all relevant issues and populate the cache in the same schema used by the webhook pipeline.\n\n## Description\n\nImplement a bootstrap mechanism to load existing Jira issues into the DynamoDB cache so that the very first audits have complete data coverage.  \n\n- Run JQL queries (scoped by project, fixVersion, or release labels).  \n- Normalize results into schema:  \n  `{issue_key, fields, changelog, updated_at, version}`  \n- Store normalized items into the DynamoDB table.  \n- Support resumable execution (checkpointing).  \n- Provide summary reporting at completion.  \n\n## Acceptance Criteria\n\n- [ ] CLI or Lambda job runs JQL queries and writes results to DynamoDB.  \n- [ ] Data written in the same schema as webhook ingestion (PR #80).  \n- [ ] Job supports checkpointing to resume after interruption.  \n- [ ] Structured logging and retry/backoff logic reused from PR #79.  \n- [ ] Summary report displays issues loaded, skipped, and failed.  \n- [ ] First audit can run successfully with complete dataset.  \n\n## Codex Prompt\n\nYou are a senior Python engineer with Jira API experience.  \n\n**Issue Reference:** Initial Bootstrap Load via JQL → Cache Store (#102)  \n\nYour task is to implement a bootstrap mechanism to seed the Jira cache with existing issues before the first audits run.  \n\n### Core Tasks\n1. Implement a CLI or Lambda job that fetches Jira issues via JQL in pages.  \n2. Normalize each issue into schema `{issue_key, fields, changelog, updated_at, version}`.  \n3. Write normalized issues into the DynamoDB cache used by webhook ingestion.  \n4. Add checkpointing to resume after failure or interruption.  \n5. Apply structured logging and retry/backoff utilities introduced in PR #79.  \n6. Print a clear summary report (loaded, skipped, failed counts).  \n\n### Acceptance Criteria\n- DynamoDB cache populated with all Jira issues in release scope.  \n- Job can be safely resumed after a partial run.  \n- Schema matches webhook ingestion pipeline (PR #80).  \n- First audit runs against a complete, correct dataset.  ","number":102,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"[Jira] Initial Bootstrap Load via JQL → Cache Store","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/102"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfLsqo","labels":["cli","data"],"milestone":{"description":"Graph + Multimodal RAG + Project Memory","dueOn":"","title":"Release Copilot v2.0"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Backlog","title":"[Jira] Initial Bootstrap Load via JQL → Cache Store"},{"content":{"body":"PR #66 added optional EventBridge schedules, and PR #58 added CloudWatch alarms. This issue formalizes reconciliation: ensuring cached Jira data matches ground truth via JQL.\n\n### Acceptance Criteria\n- EventBridge nightly trigger runs Lambda reconciliation job.\n- Job compares sampled JQL results against DynamoDB entries.\n- Drift report generated: missing, outdated, inconsistent.\n- Auto-healing configurable (repair vs report only).\n- Metrics emitted (drift count, repairs attempted).\n- Alerts reused from PR #58 (CloudWatch alarms).\n\n### Codex Prompt\nYou are a senior SRE specializing in AWS.  \n\n**Issue Reference: Nightly Reconciliation (EventBridge) for Drift**  \n\nImplement a nightly drift reconciliation between Jira and the DynamoDB cache.  \n\n**Core Tasks**\n1. Add CDK EventBridge rule to run nightly (extend PR #66).\n2. Lambda function:\n   - Fetch JQL slice from Jira.\n   - Compare with cached items in DynamoDB.\n   - Generate drift report.\n   - Optionally repair drift.\n3. Publish metrics (drift detected, drift healed).\n4. Integrate CloudWatch alarms from PR #58.\n\n**Acceptance Criteria**\n- Nightly drift report generated.\n- Cache auto-heals if enabled.\n- Alerts fire if drift exceeds threshold.","number":95,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Nightly Reconciliation (EventBridge) for Drift","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/95"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfLsqk","labels":["infra","security","testing"],"milestone":{"description":"Graph + Multimodal RAG + Project Memory","dueOn":"","title":"Release Copilot v2.0"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Backlog","title":"Nightly Reconciliation (EventBridge) for Drift"},{"content":{"body":"PR #79 added retries and structured errors. This task extends reliability with DLQ, poison-pill handling, and replay.\n\n### Acceptance Criteria\n- Retry/backoff policies applied consistently (building on PR #79).\n- DLQ (SQS) attached to webhook Lambda.\n- Malformed events quarantined (poison-pill detection).\n- CLI or Lambda tool to replay DLQ items.\n- CloudWatch alarms added for DLQ depth, error rate, latency.\n\n### Codex Prompt\nYou are a senior platform engineer.  \n\n**Issue Reference: Guardrails: Retries, DLQ, Poison-pill, Alarms**  \n\nHarden the webhook ingestion system with recovery mechanisms.  \n\n**Core Tasks**\n1. Extend Lambda configuration with DLQ (SQS).\n2. Implement poison-pill detection logic in Lambda.\n3. Provide replay utility for DLQ items.\n4. Add CloudWatch alarms for DLQ depth, error rate, latency.\n5. Ensure retry/backoff logic from PR #79 remains intact.\n\n**Acceptance Criteria**\n- Failed events routed to DLQ and replayable.\n- Malformed events quarantined safely.\n- Alerts fire on error thresholds.\n- Ingestion pipeline continues processing unaffected.","number":92,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Guardrails: Retries, DLQ, Poison-pill, Alarms","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/92"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfLsms","labels":["cli","testing"],"milestone":{"description":"Graph + Multimodal RAG + Project Memory","dueOn":"","title":"Release Copilot v2.0"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Backlog","title":"Guardrails: Retries, DLQ, Poison-pill, Alarms"},{"content":{"body":"## Problem\nThe **weekly-history** workflow fails to start due to an action resolution error on hosted ubuntu-24.04 runners:\n\n```\nCurrent runner version: '2.328.0'\nRunner Image: ubuntu-24.04 (20250922.53.1)\nError: Unable to resolve action `rhysd/actionlint@v1`, unable to find version `v1`\n```\n\nThis blocks the Git Historian job from linting and proceeding.\n\n---\n\n## Likely Causes\n- The tag `v1` is unavailable or not published for the `rhysd/actionlint` action in the current registry context.\n- Runner image updates changed marketplace resolution behavior.\n- The action was renamed/moved; a different action wrapper is recommended (`reviewdog/action-actionlint`), or we must **pin a concrete tag or commit SHA**.\n- Alternatively, we can **install and run actionlint** as a binary (no `uses:` step).\n\n---\n\n## Proposed Fix (choose one and pin versions)\n**Option A — Switch to the maintained wrapper (recommended):**\n```yaml\n- name: Lint workflow (actionlint)\n  uses: reviewdog/action-actionlint@v1\n  with:\n    github_token: ${{ secrets.GITHUB_TOKEN }}\n    reporter: github-pr-check\n    filter_mode: nofilter\n```\n> Pros: Well-maintained; works across Ubuntu images.  \n> Cons: Adds reviewdog wrapper (slightly different outputs).\n\n**Option B — Pin a specific rhysd/actionlint release tag or commit (if available):**\n```yaml\n- name: Lint workflow (actionlint)\n  uses: rhysd/actionlint@<tag-or-commit-sha>\n  # example: rhysd/actionlint@v1.7.0  (replace with an existing tag)\n```\n> Pros: Minimal change.  \n> Cons: Requires a valid tag; may break again if tags change. Prefer a **commit SHA** for determinism.\n\n**Option C — Install actionlint binary directly (no marketplace action):**\n```yaml\n- name: Install actionlint\n  run: |\n    set -euo pipefail\n    curl -sSL https://raw.githubusercontent.com/rhysd/actionlint/main/scripts/download-actionlint.bash       | bash -s -- -b /usr/local/bin\n    actionlint -version\n\n- name: Lint workflow files\n  run: |\n    actionlint -shellcheck= -color\n```\n> Pros: Independent of `uses:` resolution; version can be pinned by fetching a specific commit URL.  \n> Cons: Adds a few seconds to CI.\n\n---\n\n## Acceptance Criteria\n- [ ] Workflow lints successfully on **ubuntu-24.04** runners with **no resolution errors**.\n- [ ] The chosen method is **version-pinned** (exact tag or commit SHA for reproducibility).  \n- [ ] Failing lint should **fail the job**; passing lint allows subsequent steps to run.  \n- [ ] Documentation updated in `docs/git-historian.md` (or similar) with the chosen approach and how to update the pinned version.  \n- [ ] A manual **workflow_dispatch** run succeeds end-to-end on a test branch.\n\n---\n\n## Tasks\n- [ ] Implement **Option A** (recommended) or choose B/C based on team preference.  \n- [ ] Add `permissions` block if missing:\n  ```yaml\n  permissions:\n    contents: write\n    pull-requests: write\n  ```\n- [ ] Verify the `weekly-history` workflow completes on **manual dispatch**.  \n- [ ] Update docs: how we lint, how to bump versions, and how to troubleshoot action resolution failures.  \n- [ ] Add a CI badge or short note to the README for visibility.\n\n---\n\n## Rollback Plan\n- If linting still fails due to networking/registry issues, temporarily **disable lint step** with a TODO comment while continuing the rest of the job, then re-enable once fixed.\n\n---\n\n## Codex Prompt (implementation)\n> You are a DevOps engineer. Update `.github/workflows/weekly-history.yml` to fix the actionlint resolution failure on ubuntu-24.04 runners. Prefer **Option A** below unless there is an explicit dependency on `rhysd/actionlint`. Ensure the job remains minimal and deterministic by **pinning versions**.\n>\n> **Inputs & Goals**\n> - Runner: ubuntu-24.04\n> - Error: “Unable to resolve action `rhysd/actionlint@v1`”\n> - Keep the rest of the historian steps intact (generate history, commit/PR, etc.).\n> - Add a `permissions` block if missing.\n>\n> **Steps**\n> 1. Replace the lint step with **one** of:\n>    - **A (recommended):** `uses: reviewdog/action-actionlint@v1` and configure it to check all YAML under `.github/workflows/`.\n>    - **B:** `uses: rhysd/actionlint@<valid tag or commit SHA>` — verify the tag exists; otherwise pick commit SHA.\n>    - **C:** Install and run actionlint binary via curl script; run `actionlint -color`.\n> 2. Pin versions (tag or commit SHA).\n> 3. Run a dry run locally or in CI with `workflow_dispatch` to confirm success.\n>\n> **Expected Output**\n> - A patched `weekly-history.yml` with the working lint step (A/B/C), pinned version, and clear comments.\n> - Confirmation message about successful run.\n>\n> **Example (Option A)**\n> ```yaml\n> - name: Lint workflow (actionlint)\n>   uses: reviewdog/action-actionlint@v1\n>   with:\n>     github_token: ${{ secrets.GITHUB_TOKEN }}\n>     reporter: github-pr-check\n>     filter_mode: nofilter\n> ```\n>\n> **Notes**\n> - If you retain a `uses: rhysd/actionlint@...` step, verify the tag exists and consider pinning a **commit SHA** to avoid future tag drift.\n> - Ensure lint failures fail the job (`continue-on-error: false`).\n\n---\n\n## Verification\n- Trigger **workflow_dispatch** on a branch.\n- Confirm the lint step runs and reports results.\n- Confirm the rest of the historian flow (generation + commit/PR) completes.","number":107,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Git Historian CI: fix `rhysd/actionlint@v1` resolution error on ubuntu-24.04 runners","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/107"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfLZLo","labels":["bug","documentation","infra","high-priority","testing"],"linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/108"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"Git Historian CI: fix `rhysd/actionlint@v1` resolution error on ubuntu-24.04 runners"},{"content":{"body":"## Summary\n\nThe weekly Git Historian workflow fails when attempting to create a pull request:\n\n```\nError: GitHub Actions is not permitted to create or approve pull requests.\n```\n\nThe action commits the snapshot branch successfully (`auto/history-2025-09-26`), but PR creation is blocked by repository policy.\n\n---\n\n## Solution Approach (Recommended)\n\n**Manual + YAML Update**\n\n1. **Manual GitHub Setting (one-time change)**\n   - Navigate to: **Repo → Settings → Actions → General**\n   - Under **Workflow permissions**:\n     - Select: **Read and write permissions**\n     - Check: **[x] Allow GitHub Actions to create and approve pull requests**\n\n2. **YAML Update (small code change)**\n   - Ensure the workflow declares the required permissions:\n\n```yaml\npermissions:\n  contents: write\n  pull-requests: write\n```\n\nThis ensures the `GITHUB_TOKEN` has the scopes needed to create pull requests.\n\n---\n\n## Why This Works\n\n- **Policy fix:** The repo setting unblocks PR creation by Actions.\n- **Least privilege:** Only `contents` and `pull-requests` write scopes are granted, limiting bot capabilities.\n- **Low maintenance:** No need for additional secrets or PATs. Uses the built-in `GITHUB_TOKEN` securely.\n\n---\n\n## Acceptance Criteria\n\n- [ ] Repo setting is updated to allow Actions to create/approve PRs.  \n- [ ] Workflow YAML includes `permissions: contents: write, pull-requests: write`.  \n- [ ] Weekly Git Historian job successfully creates a PR on schedule and via manual dispatch.  \n- [ ] Documentation is updated in `docs/runbooks/history-pr.md` with the manual repo setting steps.  \n\n---\n\n## Example Workflow Snippet\n\n```yaml\nname: weekly-history-snapshot\n\non:\n  schedule:\n    - cron: \"0 11 * * FRI\"  # 4am PT Fridays\n  workflow_dispatch:\n\npermissions:\n  contents: write\n  pull-requests: write\n\njobs:\n  history:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Generate snapshot\n        run: |\n          python scripts/generate_history.py\n\n      - name: Create PR\n        uses: peter-evans/create-pull-request@v6\n        with:\n          token: ${{ secrets.GITHUB_TOKEN }}\n          commit-message: \"chore(history): add snapshot for 2025-09-26\"\n          committer: \"Release Copilot Bot <actions@users.noreply.github.com>\"\n          author: \"Release Copilot Bot <actions@users.noreply.github.com>\"\n          branch: \"auto/history-2025-09-26\"\n          title: \"chore: weekly history snapshot 2025-09-26\"\n          body: |\n            Automated history snapshot generated by the Git Historian.\n            - Since: 10d\n            - Template: docs/history/HISTORY_TEMPLATE.md\n          add-paths: |\n            docs/history\n            docs/context/context-index.json\n          signoff: false\n          delete-branch: false\n          draft: false\n```\n\n---\n\n## Testing & Verification\n\n- Trigger workflow manually (`workflow_dispatch`) to verify PR creation.  \n- Confirm PR is created with correct title/body and changes.  \n- Allow next scheduled run to verify automation continues to work.  \n\n---\n\n## Codex Prompt\n\n**Role:** GitHub Actions expert with strong security practices.  \n\n**Task:** Update the workflow so the Git Historian can automatically open pull requests without human intervention.  \n\n**Requirements:**\n- Add `permissions: contents: write, pull-requests: write` to the YAML.  \n- Use built-in `GITHUB_TOKEN` securely.  \n- Document one-time repo setting change in runbook.  \n- Verify job creates PR successfully after fix.  \n\n**Definition of Done:** The workflow automatically creates PRs for weekly snapshots, no manual steps required after the one-time repo setting change.  \n\n---\n\n## Your Next Step →\n- Apply the manual GitHub repo setting (once).  \n- Add `permissions` block to the workflow YAML.  \n- Test via `workflow_dispatch`.\n","number":109,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Enable GitHub Actions to Create PRs for Git Historian","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/109"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfLm6A","labels":["bug","codex","infra","high-priority"],"linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/110","https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/113"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"Enable GitHub Actions to Create PRs for Git Historian"},{"content":{"body":"## Description\nImplement the **Git Historian** to automatically generate weekly (and on-demand) history check-ins for Release Copilot.  \nThe historian consolidates:\n- Closed GitHub issues & merged PRs since the last snapshot\n- Key CI/CD runs and links\n- Notable decisions (ADRs) and rationale\n- Artifacts & traceability (Excel/JSON report S3 keys, optional hashes)\n- Optional Jira ↔ commit linkage (JQL + regex on commit messages)\n\nThis provides a **human-readable snapshot** (Markdown in `docs/history/`) and a **machine-friendly index** (`docs/context/context-index.json`) so the project remains easy to develop, maintain, and refine.\n\n---\n\n## Background / Why\n- Creates a living timeline of progress and decisions\n- Improves release auditability and onboarding\n- Powers future RAG/LLM context for rapid retrieval\n\n---\n\n## Scope\n- Use the provided `scripts/generate_history.py` to draft `docs/history/YYYY-MM-DD-checkin.md`\n- Add/confirm a GitHub Actions workflow (`.github/workflows/weekly-history.yml`) that opens a PR with that draft on a schedule and manual dispatch\n- Update docs to explain how to run the historian locally and via CI\n- (Optional) Enrich with Jira story linkage and Bitbucket commit metadata\n\n---\n\n## Acceptance Criteria\n- [ ] Running `scripts/generate_history.py --since 7d --output docs/history` creates a check-in file with sections: **Completed**, **In Progress**, **Upcoming**, **Notes & Decisions**, **Artifacts & Traceability**  \n- [ ] `.github/workflows/weekly-history.yml` exists and, on schedule or manual run, opens a PR with any new/changed `docs/history/*.md` files  \n- [ ] `docs/history/README.md` links to **Git Historian** docs and template usage  \n- [ ] A new doc page **Git Historian (How to Run)** is added under `docs/` and covers: local run, manual workflow dispatch, schedule behavior, and how to customize\n- [ ] (Optional) Jira-commit linkage documented and toggled via config or env vars\n- [ ] (Optional) S3 report paths added to **Artifacts & Traceability** in the generated check-in\n- [ ] (Optional) sha256 of artifacts shown when available\n\n---\n\n## Tasks\n- [ ] Add/confirm workflow: `.github/workflows/weekly-history.yml`\n- [ ] Commit generator: `scripts/generate_history.py`\n- [ ] Add documentation: `docs/history/README.md` and `docs/git-historian.md`\n- [ ] Test **local** generation (no network changes): verify issue & PR collection\n- [ ] Test **CI** run (workflow_dispatch): confirm PR opens with a new check-in\n- [ ] Wire links from GitHub Project board to the generated check-ins\n- [ ] (Optional) Extend generator for Jira/Bitbucket enrichment\n- [ ] (Optional) Add hashes and S3 pointers to artifacts\n\n---\n\n## Out-of-Scope (for this issue)\n- Building a dashboard UI for history browsing\n- Persisting history to a separate data store (DB) beyond Git\n\n---\n\n## References\n- `docs/history/HISTORY_TEMPLATE.md` (section structure)\n- `scripts/_examples/sample_history_2025-09-24.md` (example output)\n- `.github/workflows/weekly-history.yml` (scheduled & manual runs)\n\n---\n\n## Codex Prompt (for implementation & refinement)\n**Goal:** Implement and refine the **Git Historian** that drafts weekly history snapshots and opens an automated PR.\n\n**Inputs to consider:**\n- Repo: `<owner>/<repo>`\n- Time window: `--since 7d` (default), ISO timestamps supported\n- GitHub REST API token: `${{ secrets.GITHUB_TOKEN }}` in CI\n- Sections: Completed (closed issues + merged PRs), In Progress (open issues with `in-progress` label), Upcoming (label `next-up`), Notes & Decisions (latest ADRs), Artifacts (S3 paths + optional sha256)\n\n**Requirements:**\n1. Use `scripts/generate_history.py` as the entrypoint. Accept `--since` and `--output`.\n2. Generate `docs/history/YYYY-MM-DD-checkin.md` using `HISTORY_TEMPLATE.md` fields.\n3. In CI (`weekly-history.yml`):\n   - Run the generator\n   - Branch name: `auto/history-YYYY-MM-DD`\n   - Commit only if there are changes; open a PR with a short description\n4. Keep output **scannable** (< 200 lines). Prefer links to logs.\n5. Optional enrichments behind flags/env vars:\n   - `HISTORIAN_ENABLE_JIRA=true`\n   - `HISTORIAN_ENABLE_S3_ARTIFACTS=true`\n   - `HISTORIAN_ENABLE_HASH=true`\n\n**Deliverables:**\n- Working generator script & workflow\n- Docs page “Git Historian (How to Run)” with local/CI instructions\n- Example generated check-in committed to `docs/history/`\n\n**Test Plan:**\n- Run locally with a personal access token to simulate CI\n- Trigger manual workflow; verify PR\n- Validate formatting sections, links, and optional enrichments","number":83,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Git Historian: automated weekly history snapshots (issues/PRs, decisions, artifacts)","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/83"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfLHnQ","labels":["documentation","enhancement","infra","high-priority","testing"],"linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/104"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"Git Historian: automated weekly history snapshots (issues/PRs, decisions, artifacts)"},{"content":{"body":"Introduce structured logging across the project and implement consistent error handling. Use a central logger (e.g., Python logging or loguru) with levels for debug, info, warning, error, and critical. Add retry logic with exponential backoff for network calls (Jira, Bitbucket, AWS). This will improve observability and resilience.\n\nAcceptance Criteria:\nGlobal logger initialized with consistent format (timestamp, level, message, module)\nLogging added to Jira and Bitbucket clients for request/response lifecycle\nErrors captured and logged with actionable context (e.g., JQL query, repo name)\nRetry logic added for transient network failures\n\n# Codex Prompt — Structured Logging + Error Handling + Retry\n\nYou are enhancing observability and resilience in **ReleaseCopilot-AI**.\n\n## Goals\n- Introduce **structured logging** via a central logger (Python `logging` or `loguru`), with consistent format & levels.\n- Add **consistent error handling** that captures actionable context (e.g., JQL, repo, HTTP status).\n- Implement **retry logic with exponential backoff** for transient network calls (Jira, Bitbucket, AWS).\n- Keep the change focused—no broad refactors.\n\n## Deliverables\n1. **Central logger module**\n   - Add `src/releasecopilot/logging_config.py` exposing:\n     - `get_logger(name: str) -> logging.Logger`\n     - Initialization that runs once (idempotent), setting:\n       - Format: timestamp (ISO8601), level, module, message, correlation ID (if present).\n       - Levels: DEBUG, INFO, WARNING, ERROR, CRITICAL.\n       - StreamHandler to stdout.\n       - Optional JSON format behind env flag `RC_LOG_JSON=true`.\n     - Redaction filter to avoid printing secrets.\n\n2. **Wire global logger**\n   - In entrypoints (`src/releasecopilot/cli.py`, `src/cli/main.py`, `main.py`), import and call `get_logger(__name__)`.\n   - Add support for `--log-level` CLI arg (default INFO).\n\n3. **Client logging (Jira & Bitbucket)**\n   - Add request lifecycle logs at DEBUG.\n   - On response: log status code, elapsed ms, rate-limit headers.\n   - On error: log actionable context (JQL, repo, HTTP status, snippet).\n\n4. **Retry logic**\n   - Use `tenacity` or custom decorator.\n   - Retry on 429, 5xx, timeouts, throttling exceptions.\n   - Exponential backoff with jitter, max attempts = 5.\n   - Respect `Retry-After` header.\n   - Log retries at WARNING.\n\n5. **Consistent error handling**\n   - Add `errors.py` with narrow exceptions (`JiraQueryError`, etc.).\n   - Wrap exceptions and log once at boundary.\n   - CLI exits non-zero with short actionable message.\n\n6. **Correlation ID**\n   - Generate run-scoped correlation ID (UUID).\n   - Inject into logs.\n   - Allow override via env `RC_CORR_ID`.\n\n7. **Tests**\n   - Verify log format and correlation ID.\n   - Verify secrets redaction filter.\n   - Mock HTTP to assert retry behavior.\n   - Assert clients log lifecycle and raise typed errors.\n\n8. **Docs**\n   - Update `docs/observability.md` with:\n     - Log levels, JSON mode, correlation ID.\n     - Retry behavior and backoff policy.\n     - How to disable retries.\n\n## Constraints\n- No secrets in logs.\n- No broad refactors.\n- Retries only on transient errors.\n- Preserve CLI behavior.\n\n## Acceptance Criteria\n- Global logger with consistent format.\n- Clients log lifecycle and errors.\n- Retries present, exponential backoff with jitter.\n- Redaction filter works.\n- Tests pass: `pytest -q`.\n- CI (`ruff`, `pytest`, `cdk synth`) passes.\n- Documentation updated.\n\n## Starter Snippets\n```python\n# logging_config.py\nimport logging, os, sys\n\ndef get_logger(name: str) -> logging.Logger:\n    lvl = os.getenv(\"RC_LOG_LEVEL\", \"INFO\").upper()\n    logger = logging.getLogger(name)\n    if not logger.handlers:\n        h = logging.StreamHandler(sys.stdout)\n        h.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s %(name)s %(message)s\"))\n        logger.addHandler(h)\n        logger.setLevel(getattr(logging, lvl, logging.INFO))\n    return logger\n```\n\n```python\n# retry with tenacity\nfrom tenacity import retry, stop_after_attempt, wait_exponential_jitter\n\n@retry(stop=stop_after_attempt(5), wait=wait_exponential_jitter(2, 30))\ndef fetch():\n    ...\n```\n\n## Runbook\n```bash\n# Local debug\nexport RC_LOG_LEVEL=DEBUG\npytest -q\n\n# CI (already runs):\nruff check .\npytest -q\ncd infra/cdk && cdk synth\n```\n","number":13,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Logging + error handling","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/13"},"id":"PVTI_lAHOAGJYZc4BDSLLzge0Mig","labels":["enhancement","infra"],"linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/79"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"Logging + error handling"},{"content":{"body":"**Labels:** `jira`, `infra`, `enhancement`, `high-priority`  \n**Milestone:** Memory + RAG Reporting  \n\n---\n\n## Prompt for Codex\n\n**Title:** Design & Implement a Webhook-Based Jira Data Sync Architecture for ReleaseCopilot-AI\n\n**Role:**  \nYou are a Senior Cloud Architect and DevOps Engineer with deep expertise in AWS serverless architectures, event-driven pipelines, and resilient data synchronization.\n\n**Goal:**  \nRedesign how ReleaseCopilot-AI interacts with Jira by eliminating the fragile, on-demand dependency on the Jira Cloud REST API. Instead, create a webhook-based, asynchronous data sync that maintains a local, reliable copy of Jira issues for audits. This ensures resilience against Jira outages and faster, more consistent audits.\n\n---\n\n## Problem Statement\n\nCurrently, ReleaseCopilot-AI depends on Jira’s `/search/jql` endpoint for every audit. This pull-based model is slow and unreliable, especially due to ongoing Jira Cloud API instability (e.g., [JRACLOUD-94876](https://jira.atlassian.com/browse/JRACLOUD-94876)). If Jira is down or returns errors, the entire audit process is blocked. This threatens the core mission: correlating Jira stories with Bitbucket commits and producing accurate release reports.\n\n---\n\n## Proposed Solution: Push-Based Webhook Model\n\n### Phase 1: Initial Data Ingestion\n- **Task:** Populate a baseline dataset of Jira issues.  \n- **Implementation:**  \n  - Use the existing JQL + S3/manual CSV fallback as a one-time import job.  \n  - Store results in a new local persistent database.  \n\n---\n\n### Phase 2: Real-Time Data Sync\n- **Task:** Replace on-demand Jira queries with webhook-driven updates.  \n- **Implementation:**  \n  - Configure Jira Cloud webhooks to send `issue_created`, `issue_updated`, and `issue_deleted` events.  \n  - Provision a new **AWS API Gateway endpoint** with an attached **Lambda** as the webhook listener.  \n  - Parse and validate incoming webhook payloads.  \n  - Update corresponding records in the local database (upserts, deletes).  \n  - Ensure idempotency (handle duplicate events gracefully).  \n  - Include monitoring/logging to capture failed or malformed events.  \n\n---\n\n### Phase 3: Local Persistent Storage\n- **Task:** Maintain a reliable single source of truth for Jira data.  \n- **Implementation:**  \n  - Use **Amazon DynamoDB** (serverless, scalable, fully managed).  \n  - Partition key: `issue_id`.  \n  - Secondary indexes (if needed) for queries by fixVersion, status, or assignee.  \n  - Support efficient lookups for release audit queries.  \n\n---\n\n## Redefining the Jira Agent\n\n**Old Role:**  \nDirectly query Jira Cloud REST API for each audit run.  \n\n**New Role:**  \nQuery the local **DynamoDB** dataset for audit input.  \n- Use Jira Cloud only for the one-time initial ingestion.  \n- Optionally fall back to Jira Cloud only if catastrophic database loss occurs.  \n\nThis makes audits **fast, reliable, and decoupled** from Jira availability.  \n\n---\n\n## Acceptance Criteria\n\n✅ Jira webhook events (`issue_created`, `issue_updated`, `issue_deleted`) are successfully delivered to ReleaseCopilot-AI.  \n✅ API Gateway + Lambda parse and persist data into DynamoDB with full idempotency.  \n✅ DynamoDB table accurately reflects current Jira state.  \n✅ The Jira Agent runs a release audit **using only local DynamoDB data**, with no reliance on Jira Cloud during normal operations.  \n✅ Observability in place (logs, CloudWatch metrics, error handling).  \n\n---\n\n## Codex Task\n\nImplement the full webhook-based architecture in code and IaC (CDK).  \n- Define CDK constructs for API Gateway, Lambda, and DynamoDB table.  \n- Write Lambda handler code (Python) for webhook parsing, validation, and persistence.  \n- Add retry and error handling with exponential backoff for transient failures.  \n- Provide unit tests + integration tests (mock Jira payloads).  \n- Update the Jira Agent to query DynamoDB instead of Jira API.  \n\n---\n","number":78,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Design & Implement a Webhook-Based Jira Data Sync Architecture for ReleaseCopilot-AI","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/78"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfIHUw","labels":["enhancement","jira","infra","high-priority"],"linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/80"],"milestone":{"description":"","dueOn":"","title":"Memory + RAG Reporting"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"Design & Implement a Webhook-Based Jira Data Sync Architecture for ReleaseCopilot-AI"},{"content":{"body":"# Codex Prompt — Fix Git Historian Workflow YAML Error\n\n**Context**  \nThe GitHub Actions workflow `.github/workflows/weekly-history.yml` fails to parse:\n\n```\nInvalid workflow file: .github/workflows/weekly-history.yml#L69\nYou have an error in your yaml syntax on line 69\n\nL69 commit-message: chore: add history snapshot for ${{ steps.meta.outputs.date }}\n```\nThe workflow name/PR was: `feat: add git historian automation #1`.\n\n---\n\n## Your Role\nYou are a DevOps engineer fixing a GitHub Actions YAML parsing error and validating the entire workflow end-to-end for the **Git Historian** job that generates and commits a weekly history check-in.\n\n---\n\n## Goals\n1. **Resolve the YAML syntax error** on line 69.\n2. **Validate inputs** for the commit/PR step according to the action being used (avoid wrong input names).\n3. **Ensure the workflow runs to completion** on manual dispatch and on schedule.\n4. **Add a quick lint step** to catch YAML issues early.\n\n---\n\n## Likely Root Causes & Fixes\n- **Unquoted string with colon and expression**: `commit-message: chore: add ... ${{ ... }}` should be **quoted**. YAML can misinterpret `:` in unquoted strings.\n- **Wrong input name** for the chosen action (e.g., `commit-message` vs. `commit_message`). For example:\n  - `stefanzweifel/git-auto-commit-action@v5` ⇒ **`commit_message`** (snake_case).  \n  - Other actions may vary; **use the correct input name** for the action in the workflow.\n- **Indentation**: Confirm the `with:` section indentation is 2 spaces under the step, and inputs are 2 spaces under `with:`.\n\n---\n\n## Required Changes\n1. **Quote the commit message** and **use the correct input name**. Below are two corrected patterns. Use **A** if we use `stefanzweifel/git-auto-commit-action@v5` (recommended). Use **B** only if the workflow uses a different action that expects `commit-message`.\n\n### A) Recommended (git-auto-commit-action v5)\n```yaml\n- name: Commit & push history\n  uses: stefanzweifel/git-auto-commit-action@v5\n  with:\n    commit_message: \"chore(history): add snapshot for ${{ steps.meta.outputs.date }}\"\n    branch: \"auto/history-${{ steps.meta.outputs.date }}\"\n    file_pattern: \"docs/history/*.md\"\n```\n\n### B) If using an action that expects `commit-message` (less common)\n```yaml\n- name: Commit & push history\n  uses: some/commit-action@vX\n  with:\n    commit-message: \"chore(history): add snapshot for ${{ steps.meta.outputs.date }}\"\n    branch: \"auto/history-${{ steps.meta.outputs.date }}\"\n    file-pattern: \"docs/history/*.md\"\n```\n\n> **Note:** If your existing workflow previously used raw `git` commands and `gh pr create`, keep that approach OR switch fully to an action that handles commits. If you keep raw git + GH CLI, ensure `gh` is installed and authenticated (`GITHUB_TOKEN`) and still **quote any strings with `:`**.\n\n2. **Ensure `steps.meta.outputs.date` is defined** earlier, e.g.:\n```yaml\n- name: Compute date\n  id: meta\n  run: echo \"date=$(date +%F)\" >> \"$GITHUB_OUTPUT\"\n```\n\n3. **(Optional) Add `actionlint`** to catch YAML issues early:\n```yaml\n- name: Lint workflow (actionlint)\n  uses: rhysd/actionlint@v1\n```\n\n4. **Permissions** (if using `git-auto-commit-action` or raw git pushes):\n```yaml\npermissions:\n  contents: write\n  pull-requests: write\n```\n\n5. **Guard no-change scenario** (if using raw git):\n```bash\nif git diff --cached --quiet; then\n  echo \"No changes to commit.\"\n  exit 0\nfi\n```\n\n---\n\n## Deliverables\n- Updated `.github/workflows/weekly-history.yml` with:\n  - Quoted commit message line\n  - Correct input name for the chosen action\n  - `meta` step that sets the `date` output\n  - Proper permissions\n  - (Optional) `actionlint` step\n- A successful dry run via **workflow_dispatch**.\n\n---\n\n## Acceptance Criteria\n- Workflow **parses** (no YAML errors).\n- On manual run, it **creates/updates** `docs/history/YYYY-MM-DD-checkin.md`, commits to `auto/history-YYYY-MM-DD`, and **opens a PR** (depending on approach).  \n- If nothing changed, the job exits cleanly without error.\n\n---\n\n## Example Minimal Working Block (using `git-auto-commit-action`)\nReplace your commit step with this, keeping your earlier generation step intact:\n```yaml\n- name: Commit & push history\n  uses: stefanzweifel/git-auto-commit-action@v5\n  with:\n    commit_message: \"chore(history): add snapshot for ${{ steps.meta.outputs.date }}\"\n    branch: \"auto/history-${{ steps.meta.outputs.date }}\"\n    file_pattern: \"docs/history/*.md\"\n```\n\nIf you still prefer raw git + PR creation, ensure the commit message is quoted:\n```yaml\n- name: Commit & PR history\n  run: |\n    branch=\"auto/history-${{ steps.meta.outputs.date }}\"\n    git checkout -b \"$branch\"\n    git add docs/history/*.md\n    if git diff --cached --quiet; then\n      echo \"No changes to commit.\"\n      exit 0\n    fi\n    git -c user.name=\"github-actions[bot]\" -c user.email=\"41898282+github-actions[bot]@users.noreply.github.com\" commit -m \"chore(history): add snapshot for ${{ steps.meta.outputs.date }}\"\n    git push origin \"$branch\"\n    gh pr create --title \"Weekly history snapshot ${{ steps.meta.outputs.date }}\" --body \"Automated history snapshot.\"\n```\n\n---\n\n## Test Plan\n1. Trigger **workflow_dispatch** on your branch.\n2. Verify job passes **lint** and **build** stages.\n3. Confirm the new branch `auto/history-YYYY-MM-DD` exists and a PR is opened (or commit appears).\n4. Inspect the commit message and confirm it renders correctly with the date.\n\n---\n\n## Notes\n- YAML strings that contain `:` or `${{ ... }}` should be **quoted**.\n- Double-check the **action’s input names**; `commit_message` (snake_case) is common.","number":105,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Fix Git Historian Workflow YAML Error","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/105"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfLWVo","labels":["bug","high-priority"],"linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/104"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"Fix Git Historian Workflow YAML Error"},{"content":{"body":"**Type:** Bug / Build Failure  \n**Area:** infra/cdk • lambda packaging • CI  \n**Priority:** High (blocking CI)\n\n---\n\n## Summary\n\n`cdk synth` fails in CI with an asset staging error for the Jira webhook Lambda. CDK cannot locate the code asset at `infra/cdk/services/jira_sync_webhook`. This blocks synthesis and any downstream deploy/test jobs. Additionally, we’re seeing deprecation warnings we should clean up during the fix.\n\n---\n\n## Error (Trimmed)\n\n```\nValidationError: Cannot find asset at /home/runner/work/ReleaseCopilot-AI/ReleaseCopilot-AI/infra/cdk/services/jira_sync_webhook\n  at path [ReleaseCopilot-dev-Core/JiraWebhookLambda/Code/Stage] in aws-cdk-lib.AssetStaging\n...\nRuntimeError: Cannot find asset at /home/runner/work/ReleaseCopilot-AI/ReleaseCopilot-AI/infra/cdk/services/jira_sync_webhook\n```\n\n**Deprecation warnings in the same run:**\n\n- `aws-cdk-lib.aws_lambda.FunctionOptions#logRetention` (use `logGroup` instead)\n- `aws-cdk-lib.aws_dynamodb.TableOptions#pointInTimeRecovery` (use `pointInTimeRecoverySpecification` instead)\n\n---\n\n## Why this matters (Concepts)\n\nCDK “assets” are files/folders that must exist at synth time so CDK can stage and later publish them (e.g., to S3) for Lambda code, Docker images, etc. If the path is wrong, missing in the repo, or not present in CI’s working directory, **synth fails**. Fixing this also ensures our IaC remains reproducible and our CI/CD is reliable.\n\n---\n\n## Reproduction\n\n**Local & CI (Github Actions):**\n```bash\nsource .venv/bin/activate\ncd infra/cdk\ncdk synth\n```\nFails on constructing `JiraWebhookLambda` due to missing asset path.\n\n---\n\n## Suspected Root Causes (Investigate)\n\n1. **Incorrect asset path** in `core_stack.py` (e.g., `Code.from_asset(\"services/jira_sync_webhook\")`) vs actual project structure.\n2. **Missing code folder** not committed to the repo (`infra/cdk/services/jira_sync_webhook/` absent or renamed).\n3. **Packaging/build step moved** (e.g., Lambda code relocated to `src/` or packaged artifact path changed) without updating CDK.\n4. **.gitignore exclusions** accidentally ignoring the folder or artifact.\n5. **Monorepo path confusion** (running CDK from `infra/cdk` but asset path is relative to repo root or vice versa).\n6. **Case sensitivity or trailing slash** differences between dev and Linux CI.\n\n---\n\n## What to Change (Proposed Fix)\n\n- **Ensure correct, committed asset path** for the Jira webhook Lambda code and update CDK to point to it. Example patterns:\n  - If using a **folder of source**:\n    ```python\n    _lambda.Function(\n        self, \"JiraWebhookLambda\",\n        runtime=_lambda.Runtime.PYTHON_3_12,\n        handler=\"app.handler\",\n        code=_lambda.Code.from_asset(\n            path=os.path.join(BASE_DIR, \"..\", \"lambda\", \"jira_sync_webhook\")\n        ),\n        # Prefer explicit LogGroup (see deprecations below)\n    )\n    ```\n  - If using a **built artifact (zip)** from a build step, ensure the **build job runs before synth** and update the asset path to that artifact location.\n\n- **Clean up deprecations** while touching the code:\n  - Replace `logRetention` with an explicit `logs.LogGroup` and `RetentionDays` and attach its ARN to the Lambda:\n    ```python\n    lg = logs.LogGroup(self, \"JiraWebhookLogs\", retention=logs.RetentionDays.ONE_MONTH)\n    _lambda.Function(..., log_group=lg, ...)\n    ```\n  - Replace DynamoDB `pointInTimeRecovery` with `pointInTimeRecoverySpecification= dynamodb.PointInTimeRecoverySpecification(True)` (or via table props as supported by your CDK version).\n\n---\n\n## Acceptance Criteria\n\n- [ ] `cdk synth` completes locally and in CI with **no asset errors**.\n- [ ] CDK **finds and stages** the Jira webhook Lambda asset correctly.\n- [ ] Deprecation warnings for **`logRetention`** and **`pointInTimeRecovery`** are removed.\n- [ ] CI workflow step **`Run cdk synth`** is green on PRs to `develop` and `main`.\n- [ ] Short README note added in `infra/cdk/services/jira_sync_webhook/README.md` (or equivalent) documenting how the asset is built/located.\n\n---\n\n## Triage & Debug Checklist\n\n- [ ] Verify **folder exists** at expected path in the repo and in CI workspace (`ls -la infra/cdk/services`).\n- [ ] Confirm **relative path** used in `Code.from_asset()` resolves correctly from `infra/cdk` working directory.\n- [ ] Check `.gitignore` / `.dockerignore` for patterns excluding `infra/cdk/services/jira_sync_webhook` or build outputs.\n- [ ] If using a pre-built artifact, ensure the **build step runs before synth** and outputs to the path referenced by CDK.\n- [ ] Validate **case sensitivity** (e.g., `Jira_Sync_Webhook` vs `jira_sync_webhook`).\n\n---\n\n## Suggested Labels\n\n- `bug`\n- `infra`\n- `aws`\n- `high-priority`\n- `cdk`\n- `testing` (keeps CI green focus)\n- `codex` (automation assistance)\n\n> Use only the labels your repo already has; if `cdk` doesn’t exist, use `infra` + `aws`.\n\n---\n\n## Suggested Milestone\n\n- **Agent Refactor** (or whichever active milestone this CI fix should land in).\n\n---\n\n## Codex Prompt (Implementation Helper)\n\n**Role:** You are a Senior Cloud + CDK engineer.  \n**Task:** Fix the `cdk synth` failure by ensuring the Jira webhook Lambda asset path is correct and remove CDK deprecations encountered in the log.\n\n### What to do\n1. Inspect `infra/cdk/core_stack.py` (and any constructs) that define `JiraWebhookLambda`.\n2. Make the Lambda **code asset path** correct and reproducible in CI. If code was moved (e.g., to `src/`), update `Code.from_asset(...)` accordingly.\n3. If the Lambda requires a **build/package step**, add/update the GitHub Actions workflow so the build runs **before** `cdk synth`, and point CDK to the **built artifact**.\n4. Replace deprecated properties:\n   - `logRetention` → provide a `logs.LogGroup` and attach via `log_group` (or equivalent) to the Lambda.\n   - DynamoDB `pointInTimeRecovery` → use `pointInTimeRecoverySpecification` (or the current CDK idiom).\n5. Update unit tests (if any) and add a simple test that asserts the construct **resolves an existing asset path** (mock FS if needed).\n\n### Acceptance Criteria (repeat in code comments)\n- `cdk synth` passes locally and in CI.\n- No asset staging errors.\n- No deprecation warnings for `logRetention` or `pointInTimeRecovery`.\n- Minimal docs added explaining where the asset lives and how it’s built.\n\n### Context to assume\n- Python CDK v2 in a `infra/cdk` subfolder.\n- Lambda code lives either under `infra/cdk/services/jira_sync_webhook/` or under a `src/` directory—pick one, standardize, and update references.\n- CI runs `cd infra/cdk && cdk synth` after setting up venv.\n\n---\n\n## Links / Artifacts\n\n- CI job log section: “Run cdk synth” (see attached failure)\n- Repo path in error: `infra/cdk/services/jira_sync_webhook`\n\n---\n\n## Notes\n\nCleaning this now avoids future CDK major version friction and unblocks the webhook workstream that depends on this Lambda.\n","number":81,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"CDK Synth Failure: Asset Not Found for `jira_sync_webhook`","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/81"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfIPlE","labels":["bug","codex","aws","infra","high-priority","testing","cdk"],"linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/80"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"CDK Synth Failure: Asset Not Found for `jira_sync_webhook`"},{"content":{"body":"# Summary\nAdd an optional EventBridge rule to trigger the audit Lambda daily at a fixed time (Phoenix).  \nThis enables hands-off automation and ensures audit reports are refreshed regularly.\n\n---\n\n## Why This Matters\n- **Hands-off automation**: Audits run daily without manual triggers.  \n- **Operational signal**: Daily artifacts act as a “heartbeat”; failure means something broke.  \n- **Low cost, high impact**: A single Lambda run per day is inexpensive.  \n- **Versioned infrastructure**: Managed via CDK for reproducibility and review in PRs.  \n- **Timezone clarity**: Phoenix (America/Phoenix) is UTC-7 year-round.  \n  Example: 6:30 PM Phoenix = 01:30 UTC → `cron(30 1 * * ? *)`.\n\n---\n\n## Implementation\n1. Add CDK context parameters:\n   - `scheduleEnabled` (bool, default false)\n   - `scheduleCron` (string, default `cron(30 1 * * ? *)`)\n2. In `core_stack.py`, conditionally create an EventBridge rule:\n   ```python\n   from aws_cdk import aws_events as events, aws_events_targets as targets\n\n   if self.node.try_get_context(\"scheduleEnabled\"):\n       cron_expr = self.node.try_get_context(\"scheduleCron\") or \"cron(30 1 * * ? *)\"\n       rule = events.Rule(\n           self, \"DailyAuditTrigger\",\n           schedule=events.Schedule.expression(cron_expr)\n       )\n       rule.add_target(targets.LambdaFunction(audit_lambda))\n   ```\n3. Ensure disabling `scheduleEnabled` removes the rule.  \n4. Scheduled runs should create new timestamped S3 artifacts.\n\n---\n\n## Acceptance Criteria\n- EventBridge rule appears in AWS console when enabled.  \n- Turning the flag off removes the rule.  \n- Scheduled invocation produces new timestamped S3 objects under `reports/` and `raw/`.  \n\n---\n\n## Codex Prompt\nYou are implementing an **optional EventBridge trigger** for the ReleaseCopilot Lambda.\n\n### Deliverables\n- Modify `infra/cdk/core_stack.py` to conditionally create an EventBridge rule.  \n- Use CDK context params `scheduleEnabled` and `scheduleCron`.  \n- Default cron expression: `cron(30 1 * * ? *)` (6:30 PM Phoenix).  \n- Attach the rule target to the audit Lambda.  \n- Update `cdk.json` with defaults.  \n- Add test in `tests/infra/test_core_stack.py` to assert rule exists when enabled.  \n- Update `docs/aws-setup.md` with instructions for enabling/disabling schedule.\n\n### Constraints\n- Keep the feature optional and off by default.  \n- Ensure disabling cleans up the EventBridge rule.  \n\n### Acceptance\n- `cdk synth` passes with and without schedule enabled.  \n- Enabling deploys the rule.  \n- Scheduled Lambda writes to S3 as expected.  \n\n---\n\n## Next Steps\nRun:\n```bash\ncd infra/cdk\ncdk deploy -c scheduleEnabled=true -c scheduleCron=\"cron(30 1 * * ? *)\"\n```\n","number":49,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"[AWS] EventBridge: optional daily Lambda trigger (Phoenix time)","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/49"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfCVSE","labels":["aws","automation"],"linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/66"],"milestone":{"description":"","dueOn":"","title":"Daily Audit MVP"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"[AWS] EventBridge: optional daily Lambda trigger (Phoenix time)"},{"content":{"body":"**Detected:** 2025-09-24 19:49 UTC  \n**Pipeline step:** `Run pytest -q`  \n**Failure:** `tests/test_core_infra_stack.py::test_eventbridge_rule_targets_lambda_when_enabled`\n\n```\nE   AssertionError: assert template.find_resources(\"AWS::Events::Rule\") == {}\nLeft contains 1 more item:\n{'ReleaseCopilotSchedule865C6F41': { 'Type': 'AWS::Events::Rule', 'Properties': ... 'Targets': [{'Arn': {'Fn::GetAtt': ['ReleaseCopilotLambda739F6165','Arn']}, 'Id': 'Target0'}] }}\n```\n\n---\n\n## 📌 Summary\n\nThe unit test **`test_eventbridge_rule_targets_lambda_when_enabled`** asserts that **no** EventBridge rule exists when `schedule_enabled=True`. That is backwards.  \nWith `schedule_enabled=True` and a valid `schedule_cron`, the CDK stack **should** synthesize an `AWS::Events::Rule` targeting the audit Lambda. The synthesized template **correctly contains** the rule and target; the **test expectation is wrong**.\n\nThis is causing CI to fail even though the infrastructure behavior matches the feature intent (“create a daily trigger when enabled”).\n\n---\n\n## 🎯 Desired Behavior\n\n- When `schedule_enabled=True`, a single `AWS::Events::Rule` exists with:\n  - `ScheduleExpression` equal to the provided cron string.\n  - `State` set to `ENABLED`.\n  - Exactly one `Target` pointing to the audit Lambda ARN.\n\n- When `schedule_enabled=False`, **no** EventBridge rule is created.\n\n---\n\n## ✅ Acceptance Criteria\n\n1. Update `test_eventbridge_rule_targets_lambda_when_enabled` to assert **presence** and correctness of the rule & target.\n2. Add/keep a complementary test (or parametrize) verifying **absence** when `schedule_enabled=False`.\n3. `pytest -q` returns **0 failures** locally and in CI.\n4. GitHub Actions workflow **passes** on PR and on `main` after merge.\n5. Document the behavior in the CDK stack docstring/comments to prevent future regressions.\n\n---\n\n## 🧪 Repro Steps\n\n```bash\n# from repo root\npython -m venv .venv && source .venv/bin/activate   # Windows: .venv\\Scripts\\activate\npip install -r requirements-dev.txt\npytest -q\n```\n\nExpected (after fix): `1 failed -> 0 failed` (all tests pass).\n\n---\n\n## 🛠 Proposed Fix (Test)\n\nIn `tests/test_core_infra_stack.py` (line numbers approximate):\n\n```python\ndef test_eventbridge_rule_targets_lambda_when_enabled() -> None:\n    template = _synth_stack(schedule_enabled=True, schedule_cron=\"cron(30 8 * * ? *)\")\n\n    rules = template.find_resources(\"AWS::Events::Rule\")\n    assert len(rules) == 1\n\n    logical_id, rule = next(iter(rules.items()))\n    props = rule[\"Properties\"]\n\n    assert props[\"ScheduleExpression\"] == \"cron(30 8 * * ? *)\"\n    assert props[\"State\"] == \"ENABLED\"\n\n    targets = props[\"Targets\"]\n    assert isinstance(targets, list) and len(targets) == 1\n    target = targets[0]\n\n    # Validate the target is the audit Lambda\n    assert target[\"Id\"] == \"Target0\"\n    assert target[\"Arn\"] == { \"Fn::GetAtt\": [\"ReleaseCopilotLambda739F6165\", \"Arn\"] }\n```\n\nAlso ensure an explicit negative test exists:\n\n```python\ndef test_eventbridge_rule_absent_when_disabled() -> None:\n    template = _synth_stack(schedule_enabled=False)\n    assert template.find_resources(\"AWS::Events::Rule\") == {}\n```\n\n> **Learning callout:** This pattern tests **feature flags** cleanly: verify _presence_ and _shape_ when enabled; verify _absence_ when disabled. It prevents false positives like the current failure.\n\n---\n\n## 🧩 Why This Matters (Concepts)\n\n- **Contract testing for infra:** Your tests encode the intended behavior. If we say “enabled => create schedule,” the test must reflect that contract.\n- **CI signal quality:** Flaky or incorrect tests reduce trust in CI. Fixing the expectation restores signal fidelity.\n- **Traceability:** The rule enables automated daily audits that feed S3 artifacts, which become “project snapshots” used by Release Copilot’s Historian.\n\n---\n\n## 🔖 Labels\n\n- `bug` (Something isn't working)  \n- `testing`  \n- `infra`  \n- `aws`  \n- `high-priority`  \n- `codex` (work can be assisted by a prompt)\n\n---\n\n## 🎯 Milestone\n\n**Infra & CI – Lambda Scheduler**  \n> Groups work around EventBridge scheduling, CI reliability, and infra tests.\n\n---\n\n## 🧠 Codex Prompt (Drop-in)\n\nYou are an expert Python + AWS CDK (Python) developer with strong pytest skills.  \nTask: Fix the failing test for the EventBridge daily trigger in **Release Copilot**.\n\n**Context**\n- Feature: Optional daily EventBridge rule that triggers the audit Lambda.\n- Expectation: When `schedule_enabled=True` and a `schedule_cron` is provided, the synthesized template **must** include exactly one `AWS::Events::Rule` with that cron, `State=ENABLED`, and a single target pointing to the audit Lambda ARN.\n- Current failure: The test asserts `find_resources(\"AWS::Events::Rule\") == {}` which is wrong; the template correctly contains the rule.\n\n**What to do**\n1. Update `tests/test_core_infra_stack.py::test_eventbridge_rule_targets_lambda_when_enabled` to:\n   - Assert one `AWS::Events::Rule` exists.\n   - Validate `ScheduleExpression`, `State`, `Targets[0].Id`, and `Targets[0].Arn` (`Fn::GetAtt` of the audit Lambda).\n2. Ensure a negative test exists for `schedule_enabled=False` (no rule).\n3. Run `pytest -q` until all tests pass.\n4. Commit with message:\n   - `fix(tests): correct EventBridge schedule expectations when enabled`\n5. Document the behavior in the stack file (docstring/comments).\n\n**Deliverables**\n- Updated test(s) with clear assertions.\n- Passing CI run.\n\n**Notes**\n- Keep assertions resilient to logical ID fluctuations if your stack uses different names (match by Type/Properties rather than exact logical ID if needed).\n\n---\n\n## 📎 Context Maintenance\n\n- **Board:** Move this issue to *In Progress* when you open a PR; to *Done* after CI passes on `main`.\n- **Traceability:** Link PR to this issue. Reference test names and the feature flag (`schedule_enabled`) in the description.\n- **Historian Snapshot:** After merge, include the passing CI link and test diff in the weekly summary.\n\n---\n\n## ✅ Definition of Done\n\n- Tests and CI green.\n- Docs/comments updated.\n- Issue closed with a brief post-mortem note (root cause: incorrect expectation; prevention: dual positive/negative tests).\n","number":67,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"CI Failure: `pytest` fails on EventBridge schedule test (workflow error)","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/67"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfIAU0","labels":["bug","codex","aws","infra","high-priority","testing"],"linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/66"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"CI Failure: `pytest` fails on EventBridge schedule test (workflow error)"},{"content":{"body":"## Summary\nThe repository currently has two workflows:\n- `.github/workflows/ci.yml` → runs lint, tests, package, synth on push/PR/tag.\n- `.github/workflows/cdk-ci.yml` → overlaps much of the same tasks (lint/tests/synth).\n\nThis creates duplication and confusion. We need to consolidate into a **single CI workflow** that covers all jobs, adds missing features, and avoids the invalid `secrets.*` if-expression bug.\n\n## Goals\n1. **Consolidate Workflows**  \n   Merge `.github/workflows/ci.yml` and `.github/workflows/cdk-ci.yml` into one file. Remove duplication.\n\n2. **Add Manual Uploader**  \n   Support `workflow_dispatch` with inputs `run_uploader` (bool) and `fix_version` (string).  \n   If `run_uploader=true` and AWS creds are available, run the uploader CLI with flags:\n   ```bash\n   python main.py --s3-bucket $RC_S3_BUCKET --s3-prefix releasecopilot --fix-version <value>\n   ```\n\n3. **Guard Against Empty Artifacts**  \n   Fail fast if `dist/lambda_bundle.zip` is missing or empty.\n\n4. **Secrets Handling Fix**  \n   Derive flags from secrets into `env` (via `$GITHUB_ENV`) instead of referencing `secrets.*` in `if:` conditions.  \n   Example pattern:\n   ```yaml\n   - name: Compute OIDC availability flags\n     env:\n       ROLE: ${{ secrets.AWS_ROLE_TO_ASSUME }}\n     run: |\n       if [ -n \"$ROLE\" ]; then\n         echo \"AWS_ROLE_TO_ASSUME=$ROLE\" >> $GITHUB_ENV\n         echo \"OIDC_PRESENT=true\" >> $GITHUB_ENV\n       else\n         echo \"OIDC_PRESENT=false\" >> $GITHUB_ENV\n       fi\n   ```\n\n5. **Docs Update**  \n   Expand `docs/ci-cd.md` with:\n   - Triggers (push/PR/tag/manual).  \n   - How to run manual uploader.  \n   - Required repo/org secrets & variables.  \n   - Local smoke tests (`pytest`, `ruff check`, packaging helper).\n\n---\n\n## Labels\n- `infra`\n- `aws`\n- `export`\n- `documentation`\n- `enhancement`\n\n## Milestone\n- MVP CI/CD\n\n---\n\n# Codex Prompt — Consolidate CI, add manual uploader, and avoid `secrets` expression errors\n\nYou are consolidating the CI setup for ReleaseCopilot-AI.\n\n## Non-negotiable guardrails (to prevent past errors)\n- **Never** reference `secrets.*` inside any `if:` expression.  \n- **Never** put secret-based conditions at the **job level**. Keep at least one job **unconditional** so checks always run.  \n- Use the **env-flag pattern**: first copy secrets into env (via `$GITHUB_ENV`) in a step, then gate later steps with `if: ${{ env.FLAG == 'true' }}`.  \n- Keep `permissions: { contents: read, id-token: write }` where OIDC is used.\n\n## Deliverables\n1) Replace/merge into a **single** workflow: `.github/workflows/ci.yml`  \n   **Triggers:** push (`main`, `feature/**`), pull_request (`main`), tags `v*.*.*`, and `workflow_dispatch` (inputs: `run_uploader` bool, `fix_version` string).\n\n2) **Jobs**\n- `python-checks` (always runs; no secret conditions):  \n  - Python 3.11, pip cache  \n  - `ruff check .` (soft fail OK now)  \n  - `pytest -q`\n- `package` (needs: python-checks):  \n  - Build `dist/lambda_bundle.zip` using `scripts/package_lambda.py` (or `.sh`, else fallback zip)  \n  - **Guard:** `test -s dist/lambda_bundle.zip` must succeed (fail if empty)  \n  - Upload artifact on PR/push; retain on tags\n- `cdk-synth` (needs: python-checks):  \n  - Install CDK v2; create venv; `pip install -r infra/cdk/requirements.txt`  \n  - `cd infra/cdk && cdk synth`\n- `optional-uploader` (only on `workflow_dispatch` and `run_uploader=true`):  \n  - **Do not** use `secrets.*` in `if:`.  \n  - **Step A (compute flags):** read `secrets.AWS_ROLE_TO_ASSUME` into an env var and set `OIDC_PRESENT=true/false` via `$GITHUB_ENV`  \n  - **Step B (OIDC):** `if: ${{ env.OIDC_PRESENT == 'true' }}` use `aws-actions/configure-aws-credentials@v4` with `role-to-assume: ${{ env.AWS_ROLE_TO_ASSUME }}` and `aws-region` from env  \n  - **Step C (run uploader):** resolve `FIX_VERSION` (from input, or tag name, else today’s date), then call the CLI:  \n    `python -m releasecopilot.cli --fix-version \"$FIX_VERSION\" --s3-bucket \"$RC_S3_BUCKET\" --s3-prefix \"releasecopilot\"`  \n  - If `RC_S3_BUCKET` is empty → log and skip gracefully\n\n3) **Secrets handling pattern (required)**\n```yaml\n- name: Compute OIDC availability flags\n  env:\n    ROLE: ${{ secrets.AWS_ROLE_TO_ASSUME }}  # safe: no 'if:' uses 'secrets'\n  run: |\n    if [ -n \"$ROLE\" ]; then\n      echo \"AWS_ROLE_TO_ASSUME=$ROLE\" >> \"$GITHUB_ENV\"\n      echo \"OIDC_PRESENT=true\" >> \"$GITHUB_ENV\"\n    else\n      echo \"OIDC_PRESENT=false\" >> \"$GITHUB_ENV\"\n    fi\n\n# Later steps must use only env.* in conditions:\n# if: ${{ env.OIDC_PRESENT == 'true' }}\n```\n\n4) **Permissions**\n- For jobs/steps that use OIDC, set:\n```yaml\npermissions:\n  contents: read\n  id-token: write\n```\n\n5) **Docs update (`docs/ci-cd.md`)**\n- Triggers & Jobs overview (PR/push/tag/manual).  \n- OIDC setup: create AWS role that trusts your GH org/repo; save ARN as `AWS_ROLE_TO_ASSUME` secret.  \n- Required secrets/vars:\n  - `AWS_ROLE_TO_ASSUME` (secret, optional; enables OIDC path)  \n  - `RC_S3_BUCKET` (secret)  \n  - `AWS_REGION` (repo/org variable; default `us-west-2`)  \n- Manual run steps (workflow_dispatch, inputs, expected S3 paths).  \n- Local smoke tests (`ruff`, `pytest`, packaging helper).  \n- **Guardrails note:** no `secrets.*` in `if:`; prefer env-flag pattern; keep at least one always-running job.\n\n6) **CI Quick Runbook** (append to docs)\n```bash\nruff check .\npytest -q\npython scripts/package_lambda.py --out dist/lambda_bundle.zip\ncd infra/cdk && cdk synth\n```\n\n## Acceptance\n- Only **one** CI workflow remains (`ci.yml`); `cdk-ci.yml` removed.  \n- PR/push: lint/tests/package/synth run successfully.  \n- Tag: retained artifact uploaded.  \n- Manual `workflow_dispatch` with `run_uploader=true` uploads to S3 *when* OIDC role + bucket secrets exist; otherwise logs and skips.  \n- **No** “Unrecognized named-value: 'secrets'” errors (no `secrets.*` inside `if:` anywhere).\n\n---\n\n### Minimal reference snippet for the uploader job (uses env-flag pattern)\n```yaml\noptional-uploader:\n  if: ${{ github.event_name == 'workflow_dispatch' && inputs.run_uploader == true }}\n  runs-on: ubuntu-latest\n  needs: [ package ]\n  permissions:\n    contents: read\n    id-token: write\n  env:\n    AWS_REGION: ${{ vars.AWS_REGION || 'us-west-2' }}\n    RC_S3_BUCKET: ${{ secrets.RC_S3_BUCKET }}\n    RC_S3_PREFIX: \"releasecopilot\"\n    FIX_VERSION: ${{ inputs.fix_version }}\n  steps:\n    - uses: actions/checkout@v4\n\n    - name: Compute OIDC availability flags\n      env:\n        ROLE: ${{ secrets.AWS_ROLE_TO_ASSUME }}\n      run: |\n        if [ -n \"$ROLE\" ]; then\n          echo \"AWS_ROLE_TO_ASSUME=$ROLE\" >> \"$GITHUB_ENV\"\n          echo \"OIDC_PRESENT=true\" >> \"$GITHUB_ENV\"\n        else\n          echo \"OIDC_PRESENT=false\" >> \"$GITHUB_ENV\"\n        fi\n\n    - name: Configure AWS credentials (OIDC)\n      if: ${{ env.OIDC_PRESENT == 'true' }}\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        role-to-assume: ${{ env.AWS_ROLE_TO_ASSUME }}\n        aws-region: ${{ env.AWS_REGION }}\n\n    - name: Resolve fixVersion & run uploader\n      env:\n        GIT_REF_TYPE: ${{ github.ref_type }}\n        GIT_REF_NAME: ${{ github.ref_name }}\n      run: |\n        if [ -z \"$FIX_VERSION\" ]; then\n          if [ \"$GIT_REF_TYPE\" = \"tag\" ]; then\n            FIX_VERSION=\"$GIT_REF_NAME\"\n          else\n            FIX_VERSION=\"$(date +%Y.%m.%d)\"\n          fi\n        fi\n        if [ -z \"$RC_S3_BUCKET\" ]; then\n          echo \"RC_S3_BUCKET not set; skipping upload.\"; exit 0;\n        fi\n        python -m releasecopilot.cli           --fix-version \"$FIX_VERSION\"           --s3-bucket \"$RC_S3_BUCKET\"           --s3-prefix \"$RC_S3_PREFIX\"\n```\n\n","number":64,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"CI Consolidation & Uploader Workflow","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/64"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfHZLI","labels":["documentation","enhancement","export","aws","infra"],"linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/65"],"milestone":{"description":"","dueOn":"","title":"AWS Infra (CDK) MVP"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"CI Consolidation & Uploader Workflow"},{"content":{"body":"Add unit tests (matcher, exporter, config) and one integration test (sample fixtures → artifacts).\nAcceptance Criteria:\n\nTests pass in CI; coverage ≥ 70%","number":23,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Core Tests (CLI + Modules)","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/23"},"id":"PVTI_lAHOAGJYZc4BDSLLzge0hGQ","labels":["infra","testing"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"Core Tests (CLI + Modules)"},{"content":{"body":"## Summary\nAdd a baseline CI pipeline that runs lint/tests, builds the Lambda artifact, and performs a CDK synth on every PR/push; and add an S3 uploader module that pushes audit artifacts (JSON/Excel + raw payloads) to a bucket under a versioned prefix `<prefix>/<fix-version>/<YYYY-MM-DD_HHMMSS>/...`.\n\nThis formalizes our automation path and ensures every change is tested, packaged, and infra-valid before merge. It also standardizes how reports are published for UI consumption and downstream tools.\n\n## Why this matters\n- **Fast feedback:** Catch regressions early (lint/tests) and infra drift (cdk synth).\n- **Repeatable packaging:** Guarantees deployable Lambda bundles exist on demand.\n- **Consistent publishing:** S3 layout makes it easy to browse runs by fix version and timestamp; unlocks the Streamlit UI picker and “compare” workflows.\n\n## Scope\n1) **CI pipeline (GitHub Actions)**\n   - Triggers: PRs to `main`, pushes to `main` and `feature/**`, `workflow_dispatch`, tags `v*.*.*`.\n   - Jobs:\n     - `python-checks`: set up Python 3.11, install deps, run linter (ruff/flake8) + `pytest -q`.\n     - `package`: build Lambda bundle via packaging helper (fallback to zip) and upload as artifact.\n     - `cdk-synth`: install CDK and run `cdk synth` (no deploy).\n   - Caching: pip cache.\n   - Artifacts: `lambda_bundle.zip` on PRs; tagged artifacts named with the tag.\n\n2) **S3 uploader**\n   - New module: `src/releasecopilot/uploader.py` with:\n     - `build_versioned_prefix(base_prefix, fix_version, when) -> \"<base>/<fix>/<YYYY-MM-DD_HHMMSS>\"`\n     - `upload_directory(bucket, base_prefix, local_dir, subdir, metadata)`\n   - CLI flags/env:\n     - `--s3-bucket` (or `RC_S3_BUCKET`) optional.\n     - `--s3-prefix` (or `RC_S3_PREFIX`, default `releasecopilot`).\n   - Behavior:\n     - After export completes, if bucket provided, upload:\n       - `reports/` → `s3://bucket/<prefix>/<fix>/<ts>/reports/...`\n       - `raw/`     → `s3://bucket/<prefix>/<fix>/<ts>/raw/...`\n     - Use SSE-S3; add metadata: `fix-version`, `generated-at`, `git-sha` (if available).\n     - Log clear success/fail; skip gracefully if unset.\n\n## Out of scope\n- CDK deploy or environment promotion (separate release workflow).\n- Step Functions orchestration.\n\n## Acceptance Criteria\n- **CI:** On PRs/pushes, lints run, tests pass, Lambda bundle is built, and `cdk synth` succeeds.\n- **Artifacts:** `lambda_bundle.zip` downloadable from CI; tagged builds store versioned artifact.\n- **Uploader:** Running the CLI with `--s3-bucket my-bkt --s3-prefix audits --fix-version 2025.10.24` creates keys:\n  - `s3://my-bkt/audits/2025.10.24/<YYYY-MM-DD_HHMMSS>/reports/...`\n  - `s3://my-bkt/audits/2025.10.24/<YYYY-MM-DD_HHMMSS>/raw/...`\n- **Docs:** README has “CI pipeline” + “S3 upload layout” sections with examples.\n\n## Risks & mitigation\n- **IAM scope too broad:** Limit to specific bucket prefix ARNs.\n- **Timestamp nondeterminism:** Always use UTC and single timestamp for a run.\n- **Missing bucket/creds:** Skip upload with INFO log, do not fail the audit.\n\n## Implementation plan\n- Add `.github/workflows/ci.yml` (see Codex prompt below).\n- Add `src/releasecopilot/uploader.py` + minimal unit tests for prefix builder.\n- Wire new flags into CLI; ensure exporter calls uploader conditionally.\n- Update README with examples; note required AWS permissions (PutObject to prefix, SSE-S3).\n\n---\n\n## Codex Prompt\nYou are implementing CI + S3 uploader for ReleaseCopilot-AI.\n\n### Deliverables\n1. Add `.github/workflows/ci.yml` that:\n   - Triggers: PRs to `main`, pushes to `main` & `feature/**`, tags `v*.*.*`, and `workflow_dispatch` with inputs (`run_uploader`, `fix_version`).\n   - Jobs:\n     - `python-checks`: Python 3.11, pip cache, run `ruff check` and `pytest -q`.\n     - `package`: build `dist/lambda_bundle.zip` using `scripts/package_lambda.py` or `.sh` if present; fallback zip; upload artifact; **hard-fail if artifact is empty**.\n     - `cdk-synth`: install CDK v2 and synth the app in `infra/cdk`.\n     - `optional-uploader` (workflow_dispatch only): if AWS creds available, run the CLI with `--s3-bucket/--s3-prefix/--fix-version` to upload reports + raw. Prefer AWS OIDC via `aws-actions/configure-aws-credentials@v4`.\n\n2. Ensure the CLI supports:\n   - `--s3-bucket`, `--s3-prefix`, and `--fix-version` flags.\n   - Skips S3 upload gracefully if bucket not set (INFO log).\n\n3. Docs: Update `docs/ci-cd.md` with:\n   - How to run the workflow manually (`workflow_dispatch`), inputs, and expected outputs.\n   - How to set `RC_S3_BUCKET` secret and (preferred) `AWS_ROLE_TO_ASSUME` for OIDC.\n   - Example of browsing `s3://<bucket>/releasecopilot/<fix-version>/<timestamp>/...`.\n\n4. Add a short “CI Quick runbook” (commands to re-run jobs locally where feasible, e.g., `pytest`, packaging helper).\n\n5. Acceptance:\n   - CI runs lint/tests/synth on PRs and pushes.\n   - Tagged builds produce a retained artifact.\n   - Manual run with `run_uploader=true` pushes artifacts to S3 (guarded by AWS creds).\n\n---\n\n## Docs Runbook (to add in `docs/ci-cd.md`)\n\n### CI/CD Quick Runbook\n\n#### Triggers\n- PR to `main`: lint, tests, package, synth\n- Push to `main`/`feature/**`: same as PR\n- Tag `v*.*.*`: uploads a versioned artifact\n- Manual: `workflow_dispatch` inputs:\n  - `run_uploader` (bool)\n  - `fix_version` (string, e.g., 2025.10.01)\n\n#### One-time setup\n- Set repo secret `RC_S3_BUCKET` to your artifacts bucket.\n- Prefer AWS OIDC:\n  - Create an AWS IAM role that trusts your GitHub org/repo.\n  - Save its ARN as secret `AWS_ROLE_TO_ASSUME`.\n- Optional fallback secrets (not recommended): `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`.\n- Set org/repo variable `AWS_REGION` (default `us-west-2`).\n\n#### Manual run with upload\n1. Actions → CI workflow → **Run workflow**\n2. `run_uploader = true`\n3. `fix_version = 2025.10.01` (or leave empty to use tag/date)\n4. Confirm artifacts appear under:\n   ```\n   s3://$RC_S3_BUCKET/releasecopilot/<FIX_VERSION>/<YYYY-MM-DD_HHMMSS>/{reports,raw}/...\n   ```\n\n#### Local smoke tests\n```bash\nruff check .\npytest -q\npython scripts/package_lambda.py --out dist/lambda_bundle.zip  # or fallback zip command\n```\n\n---\n\n## Learning Callouts\n- **Empty artifact**: `test -s` ensures CI fails if packaging produced 0 bytes.  \n- **CDK synth**: doesn’t need AWS creds; keep deploys out of CI for now.  \n- **Uploader**: keep it **manual** via `workflow_dispatch` until you’re confident in runtime + costs.  \n- **AWS OIDC**: preferred over long-lived keys; rotate nothing, least-privilege assume-role.\n\n---\n\n## Acceptance Criteria Recap\n- CI pipeline validates code health and artifacts.\n- Tagged builds upload retained Lambda bundles.\n- Optional uploader job pushes to S3 on demand.\n","number":44,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"CI pipeline (lint → test → package → CDK synth) + S3 uploader with versioned prefixes","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/44"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfCRFY","labels":["documentation","enhancement","export","aws","infra"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"CI pipeline (lint → test → package → CDK synth) + S3 uploader with versioned prefixes"},{"content":{"body":"### Concepts (why)\n\n* **Errors alarm**: fires whenever Lambda returns an error.\n* **Throttles alarm**: fires when concurrent executions exceed limits.\n* **Email/SNS (optional)**: pushes notifications to you without opening CloudWatch.\n* All done in CDK, so it’s versioned and repeatable.\n\n### Practical steps (CDK)\n\n1.  Import `aws_cdk.aws_cloudwatch as cw`, `aws_cdk.aws_cloudwatch_actions as actions`, and (optional) `aws_cdk.aws_sns as sns`, `aws_cdk.aws_sns_subscriptions as subs`.\n2.  Create metrics bound to your Lambda function:\n    * `fn.metric_errors(period=Duration.minutes(5), statistic=\"sum\")`\n    * `fn.metric_throttles(period=Duration.minutes(5), statistic=\"sum\")`\n3.  Create alarms with sensible thresholds (e.g., >= 1 in a 5-minute window).\n4.  (Optional) Create an SNS topic + email subscription from context (e.g., `alarmEmail`).\n5.  Wire alarms to topic using `actions.SnsAction(topic)`.\n\n### Acceptance criteria\n\n* `cdk synth` shows two `AWS::CloudWatch::Alarm` resources.\n* (Optional) If `alarmEmail` provided via context, `AWS::SNS::Topic` and subscription exist.\n* Lambda log group retention still set to 30 days (from #51).\n* You can force an error to see the Errors alarm go `ALARM` in the console.\n\n---\n\n### Codex Prompt\n\nYou are adding CloudWatch alarms to the existing CDK stack in `infra/cdk/core_stack.py`.\n\n#### Goals\n\n* Create **two alarms** on the Lambda function defined in `CoreStack`:\n    1.  **Errors** ≥ 1 over 5 minutes\n    2.  **Throttles** ≥ 1 over 5 minutes\n* Keep existing **log retention = 30 days** (already set).\n* **Optional**: If a context value `alarmEmail` is provided (non-empty), create an **SNS topic** and subscribe that email; route both alarms to that topic.\n\n#### Implementation details\n\n* Use:\n    * `aws_cdk.aws_cloudwatch as cw`\n    * `aws_cdk.aws_cloudwatch_actions as actions`\n    * `aws_cdk.aws_sns as sns`\n    * `aws_cdk.aws_sns_subscriptions as subs`\n* For a Lambda `fn` created earlier in the stack, define:\n\n```python\nerrors_metric = fn.metric_errors(period=Duration.minutes(5), statistic=\"sum\")\nthrottles_metric = fn.metric_throttles(period=Duration.minutes(5), statistic=\"sum\")\n\nerrors_alarm = cw.Alarm(self, \"LambdaErrorsAlarm\",\n      metric=errors_metric,\n      threshold=1,\n      evaluation_periods=1,\n      datapoints_to_alarm=1,\n      treat_missing_data=cw.TreatMissingData.NOT_BREACHING\n)\n\nthrottles_alarm = cw.Alarm(self, \"LambdaThrottlesAlarm\",\n      metric=throttles_metric,\n      threshold=1,\n      evaluation_periods=1,\n      datapoints_to_alarm=1,\n      treat_missing_data=cw.TreatMissingData.NOT_BREACHING\n)\nif alarmEmail context exists:\ntopic = sns.Topic(self, \"ReleaseCopilotAlarmTopic\")\ntopic.add_subscription(subs.EmailSubscription(alarm_email))\nsns_action = actions.SnsAction(topic)\nerrors_alarm.add_alarm_action(sns_action)\nthrottles_alarm.add_alarm_action(sns_action)\n```\n\n#### Tests (add to tests/infra/test_core_stack.py)\n* Assert presence of two AWS::CloudWatch::Alarm resources.\n* If alarmEmail provided in a synthesized context, assert an AWS::SNS::Topic and AWS::SNS::Subscription.\n* Assert LogGroup retention remains 30 days.\n\n#### Context additions (update cdk.json)\n```json\n{\n  \"context\": {\n    \"alarmEmail\": \"\"\n  }\n}\n```\n\n#### Acceptance\n* cd infra/cdk && cdk synth shows alarms (and SNS if email set).\n* Trigger a controlled Lambda error; observe Errors alarm → ALARM.\n* (Optional) Verify an email subscription confirmation is received for SNS.\n\n#### Starter code snippet (if you’d rather paste than rely on Codex)\nAdd to your CoreStack after the Lambda is created:\n```python\nfrom aws_cdk import (\n    Duration,\n    aws_cloudwatch as cw,\n    aws_cloudwatch_actions as actions,\n    aws_sns as sns,\n    aws_sns_subscriptions as subs,\n)\n\nalarm_email = self.node.try_get_context(\"alarmEmail\") or \"\"\n\nerrors_metric = fn.metric_errors(period=Duration.minutes(5), statistic=\"sum\")\nthrottles_metric = fn.metric_throttles(period=Duration.minutes(5), statistic=\"sum\")\n\nerrors_alarm = cw.Alarm(\n    self, \"LambdaErrorsAlarm\",\n    metric=errors_metric,\n    threshold=1,\n    evaluation_periods=1,\n    datapoints_to_alarm=1,\n    treat_missing_data=cw.TreatMissingData.NOT_BREACHING,\n)\n\nthrottles_alarm = cw.Alarm(\n    self, \"LambdaThrottlesAlarm\",\n    metric=throttles_metric,\n    threshold=1,\n    evaluation_periods=1,\n    datapoints_to_alarm=1,\n    treat_missing_data=cw.TreatMissingData.NOT_BREACHING,\n)\n\nif alarm_email:\n    topic = sns.Topic(self, \"ReleaseCopilotAlarmTopic\")\n    topic.add_subscription(subs.EmailSubscription(alarm_email))\n    action = actions.SnsAction(topic)\n    errors_alarm.add_alarm_action(action)\n    throttles_alarm.add_alarm_action(action)\n```\n#### 7) Update documentation (e.g., `docs/aws-setup.md`) with a **Quick runbook** section for CloudWatch alarms:\n\n```bash\ncd infra/cdk\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\Activate\ncdk synth\n\n# deploy without email\ncdk deploy --require-approval never\n\n# deploy with email notifications\ncdk deploy --context alarmEmail=you@example.com --require-approval never\n\n# smoke test: cause a Lambda error, re-invoke, then check CloudWatch Alarms\n```\n* Note: This section must be versioned in the repository so operational steps evolve with the code.","number":50,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"[AWS] CloudWatch Logs & basic alarms (errors/throttles, retention)","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/50"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfCVZc","labels":["agent","infra","monitoring"],"linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/58"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"[AWS] CloudWatch Logs & basic alarms (errors/throttles, retention)"},{"content":{"body":"## Summary\nGH Actions workflow **`.github/workflows/cdk-ci.yml`** fails with:\n- **Invalid workflow file**: “Unrecognized named-value: `secrets`”  \n- **No jobs were run** / “there are no checks for this commit”\n\nThis typically happens when an `if:` condition references `secrets.X` **without** `${{ … }}` (e.g., `if: secrets.AWS_OIDC_ROLE_ARN != ''`) or when the only job is gated behind a broken condition. We must fix expression syntax and ensure a job always runs (with step-level conditionals for AWS auth).\n\n## What’s happening\n- GitHub Actions expressions **must** be wrapped as `${{ … }}`.  \n- If the job itself is gated incorrectly, the runner may skip **all** jobs → no checks.\n\n## Scope of fix\n1. **Always create the job**; move AWS auth gating to **step-level `if:`**.  \n2. Wrap all expressions correctly: `if: ${{ secrets.AWS_OIDC_ROLE_ARN != '' }}`.  \n3. Add RBAC for OIDC (`permissions: id-token: write`).  \n4. Provide a **fallback** path (no AWS creds) so the job still runs lint/tests/synth.  \n5. Add a quick CI doc note to prevent recurrence.\n\n---\n\n## Patch (example fix)\n\n```yaml\nname: CDK CI\n\non:\n  push:\n    branches: [ \"main\", \"feature/**\" ]\n  pull_request:\n    branches: [ \"main\" ]\n  workflow_dispatch:\n\njobs:\n  cdk-ci:\n    runs-on: ubuntu-latest\n    timeout-minutes: 20\n    permissions:\n      contents: read\n      id-token: write     # required for OIDC\n    env:\n      AWS_REGION: ${{ vars.AWS_REGION || 'us-west-2' }}\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n          cache: \"pip\"\n\n      - name: Install deps\n        run: |\n          python -m pip install --upgrade pip\n          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n          if [ -f infra/cdk/requirements.txt ]; then pip install -r infra/cdk/requirements.txt; fi\n          pip install pytest ruff\n\n      - name: Lint\n        run: ruff check . || true\n\n      - name: Unit tests\n        run: pytest -q\n\n      - name: Install CDK\n        uses: actions/setup-node@v4\n        with:\n          node-version: \"20\"\n      - run: npm i -g aws-cdk@2\n\n      - name: CDK synth\n        run: |\n          cd infra/cdk\n          cdk synth\n\n      # ✅ OIDC step only if a role is configured\n      - name: Configure AWS creds (OIDC)\n        if: ${{ secrets.AWS_OIDC_ROLE_ARN != '' }}\n        uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}\n          aws-region: ${{ env.AWS_REGION }}\n\n      - name: Note when no AWS role is set\n        if: ${{ secrets.AWS_OIDC_ROLE_ARN == '' }}\n        run: echo \"No AWS_OIDC_ROLE_ARN secret set; continuing without AWS auth.\"\n```\n\n**Key changes**\n- No job-level `if:` referencing secrets.  \n- All secret checks are **step-level** and wrapped in `${{ … }}`.  \n- `permissions.id-token: write` enabled for OIDC.  \n- Job still runs tests/synth even without AWS creds.\n\n---\n\n## Acceptance Criteria\n- Workflow loads without annotation errors.  \n- On PRs/pushes, **at least one job runs** and shows lint/tests/synth.  \n- If `AWS_OIDC_ROLE_ARN` is set, the OIDC step configures AWS creds; otherwise the workflow continues and documents the skip.  \n- The “Run failed” emails disappear for configuration reasons (failures would only be from real test/synth errors).\n\n---\n\n## Documentation update\nAdd to `docs/ci-cd.md`:\n- **Expressions must be wrapped**: use `if: ${{ secrets.MY_SECRET != '' }}`.  \n- Prefer **step-level** secret checks, not job-level gates.  \n- Ensure **at least one job is unconditional** (so checks appear on PRs).\n\n---\n\n## Labels\n`ci`, `bug`, `infra`, `cdk`, `aws`\n\n## Milestone\n`MVP CLI Pipeline`\n\n---\n\n## Codex Prompt\n```\nYou are fixing .github/workflows/cdk-ci.yml:\n\n1) Replace any job-level conditions that reference secrets with step-level `if:` guards.\n2) All expressions must be wrapped as `${{ ... }}` (e.g., `if: ${{ secrets.AWS_OIDC_ROLE_ARN != '' }}`).\n3) Ensure the workflow has at least one always-running job that executes:\n   - ruff lint\n   - pytest\n   - cdk synth (in infra/cdk)\n4) Add OIDC configuration as a step guarded by the secret:\n   - `permissions: { id-token: write, contents: read }`\n   - `aws-actions/configure-aws-credentials@v4` with `role-to-assume: ${{ secrets.AWS_OIDC_ROLE_ARN }}`\n5) Add a fallback step that echoes a message when the secret is not set, keeping the job alive.\n6) Update docs/ci-cd.md with a note about expression syntax and step-level gating.\n7) Acceptance: workflow loads (no invalid file error), job runs on PRs/pushes, and OIDC step runs only when secret exists.\n```\n","number":59,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"[CI] Fix cdk-ci.yml: invalid expression using `secrets` + ensure at least one job runs","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/59"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfGn50","labels":["bug","cli","aws","infra","cdk"],"linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/60","https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/62"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"[CI] Fix cdk-ci.yml: invalid expression using `secrets` + ensure at least one job runs"},{"content":{"body":"## What happened\nWhile merging the latest changes from `develop`, the **CDK synth** step failed in CI.\n\n**Workflow excerpt**\n```\nRun cdk synth\n  cdk synth\n  shell: /usr/bin/bash -e {0}\n  env:\n    pythonLocation: /opt/hostedtoolcache/Python/3.11.13/x64\n    PKG_CONFIG_PATH: /opt/hostedtoolcache/Python/3.11.13/x64/lib/pkgconfig\n    Python_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.13/x64\n    Python2_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.13/x64\n    Python3_ROOT_DIR: /opt/hostedtoolcache/Python/3.11.13/x64\n    LD_LIBRARY_PATH: /opt/hostedtoolcache/Python/3.11.13/x64/lib\n```\n\n**Key errors/warnings**\n```\nNode 18 has reached end-of-life... Please upgrade to a supported node version.\nThis software is currently running on node v18.20.8.\nSupported node releases: ^24, ^22, ^20, ^18 [DEPRECATED]\n...\nTraceback (most recent call last):\n  File \"/home/runner/work/ReleaseCopilot-AI/ReleaseCopilot-AI/infra/cdk/app.py\", line 49, in <module>\n    raise RuntimeError(\"CDK_DEFAULT_ACCOUNT environment variable must be set for synthesis\")\nRuntimeError: CDK_DEFAULT_ACCOUNT environment variable must be set for synthesis\n...\npython3 app.py: Subprocess exited with error 1\nError: Process completed with exit code 1.\n```\n\n---\n\n## Why this matters\n- CDK uses Node at runtime. Our CI runner is on **Node 18**, which is deprecated and will be **unsupported after 2025-11-30**.  \n- Our `infra/cdk/app.py` **hard-fails** when `CDK_DEFAULT_ACCOUNT` is not set. In CI, we didn’t export this var or configure AWS credentials/environment before running `cdk synth`.\n\n---\n\n## Scope of fix\n1. **Bump Node** in CI to **20 or 22** (LTS) to eliminate deprecation warnings now.  \n2. **Configure AWS env in CI** before `cdk synth`:\n   - Use `aws-actions/configure-aws-credentials` (OIDC) or secrets to set `AWS_REGION`, `AWS_ACCESS_KEY_ID`, etc.\n   - Export `CDK_DEFAULT_ACCOUNT` and `CDK_DEFAULT_REGION` explicitly (or let CDK resolve from the configured identity).  \n3. **Harden `app.py`**:\n   - Accept `account/region` from CDK context or `Stack.of(...)` defaults.\n   - Only require env vars if not otherwise resolved. Avoid unconditional `RuntimeError`.\n\n---\n\n## Acceptance criteria\n- [ ] GitHub Actions uses **Node ≥ 20** for CDK steps (no Node 18 notice).  \n- [ ] `cdk synth` succeeds in CI without manual environment tweaking.  \n- [ ] `infra/cdk/app.py` no longer crashes when `CDK_DEFAULT_ACCOUNT` is unset locally—falls back to context/CLI env.  \n- [ ] (Optional) Add a smoke test step that runs `cdk synth --quiet` to catch regressions.  \n- [ ] (Optional) Acknowledge CDK telemetry notice in CI (`cdk acknowledge 34892`) to keep logs clean.  \n\n---\n\n## How to reproduce locally\n```bash\n# In a clean shell without AWS env set:\nnode -v            # shows 18.x reproduces warning\ncd infra/cdk\npython3 -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\ncdk synth          # fails if CDK_DEFAULT_ACCOUNT is required by app.py\n```\n\n---\n\n## Proposed changes (high level)\n- **Workflow**: add `actions/setup-node@v4` with `node-version: '20'` (or `'22'`), add `aws-actions/configure-aws-credentials@v4` prior to `cdk synth`, export `CDK_DEFAULT_ACCOUNT/REGION` using `${{ steps.aws_creds.outputs.aws-account-id }}` when possible.  \n- **App**: update `app.py` to compute `env=Environment(account=..., region=...)` from (1) CDK context, (2) AWS SDK identity if present, else (3) `CDK_DEFAULT_*` env vars—**no hard fail** unless none are available.  \n- **Docs**: README note on local synth requirements (AWS profile or `CDK_DEFAULT_*`).  \n\n---\n\n## Links / Traceability\n- Release Copilot Infra (CDK) — relates to CI reliability for AWS-hosted path.  \n- Connects to our **Context Maintenance** policy: CI stability + clear traceability for infra changes.  \n\n---\n\n## Labels\n- `infra`\n- `aws`\n- `cli`\n- `bug`\n- `high-priority`\n\n---\n\n## Milestone\n- **Agent Refactor**  \n  _(or create “AWS Infra (CDK) MVP” if you want a more focused milestone)_\n\n---\n\n## Codex Prompt\n```markdown\nYou are implementing a fix to make `cdk synth` pass in GitHub Actions for ReleaseCopilot-AI and remove deprecated Node 18 usage.\n\n## Goals\n1) Update GitHub Actions so CDK runs on Node 20 or 22 and has AWS credentials/environment configured before `cdk synth`.\n2) Make `infra/cdk/app.py` resilient:\n   - Resolve `account` and `region` without hard failing when `CDK_DEFAULT_ACCOUNT` is unset.\n   - Prefer: (a) CDK context values from `cdk.json` or `--context`, (b) AWS identity via `boto3`/STS or CDK’s `DefaultStackSynthesizer`, (c) `CDK_DEFAULT_ACCOUNT` / `CDK_DEFAULT_REGION` env vars.\n3) Keep developer UX good locally (clear README notes). Do not introduce breaking changes.\n\n## Deliverables\n- Commit 1: Workflow update to use Node 20+, configure AWS creds, export CDK env, run `cdk synth`.\n- Commit 2: Update `infra/cdk/app.py` to resolve environment flexibly instead of hard-failing.\n- Commit 3: README update with “Synth prerequisites”.\n\n## Definition of Done\n- GitHub Actions logs show Node v20+ and `cdk synth` completes successfully.\n- No Node 18 deprecation banner appears.\n- Local developers can `cdk synth` via any of the three supported paths (context, AWS creds, or env vars).","number":56,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"CDK synth fails in CI: missing CDK_DEFAULT_ACCOUNT and deprecated Node 18 runtime","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/56"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfFN1M","labels":["bug","cli","aws","infra","high-priority"],"milestone":{"description":"","dueOn":"","title":"AWS Infra (CDK) MVP"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"CDK synth fails in CI: missing CDK_DEFAULT_ACCOUNT and deprecated Node 18 runtime"},{"content":{"body":"# Issue: Add “In Progress / Backlog / Notes / Artifacts” Collectors to Git Historian\n\n**Priority:** High — originally scheduled for v2.0, but needed immediately.  \n\n---\n\n## Codex Prompt\n\n**Role:** Senior Python + GitHub API engineer with GitHub Projects v2 + Actions experience.\n\n**Goal:** In a single PR, extend `scripts/generate_history.py` and the snapshot template to include **four new collectors**:\n\n1. **In Progress** — Issues with **Projects v2 Status = \"In Progress\"**\n2. **Backlog** — Issues with **Projects v2 Status = \"Backlog\"**\n3. **Notes & Decisions** — Comment lines containing markers (`Decision:`, `Note:`, `Blocker:`, `Action:`)\n4. **Artifacts & Traceability** — GitHub Actions artifacts (and optional S3 listing)\n\nKeep configuration in YAML. Preserve existing behavior for `Completed = merged PRs` in the date window.\n\n---\n\n## Inputs & Constraints\n\n- **Repo:** `dutchsloot84/ReleaseCopilot-AI`\n- **Script:** `scripts/generate_history.py`\n- **Output:** `docs/history/YYYY-MM-DD-checkin.md`\n- **Date window:** Already handled (`--since`, `--until`, `since_days`)\n- Use **GitHub GraphQL** for Projects v2; fallback to REST\n- **In Progress signal:** `Status = \"In Progress\"`\n- **Backlog signal:** `Status = \"Backlog\"`\n- Artifacts collector: include GitHub Actions artifacts; optional S3 support\n- Empty sections must render explicit filter/scope lines (no silent “No updates”)\n\n---\n\n## Acceptance Criteria\n\n1. Snapshot shows **four new sections** with counts:\n   - `## In Progress (N)` — issues with **Status = \"In Progress\"**\n   - `## Backlog (N)` — issues with **Status = \"Backlog\"**\n   - `## Notes & Decisions (N)` — markers captured within the date window, annotated with parent item status (Completed / In Progress / Backlog)\n   - `## Artifacts & Traceability (N)` — GitHub Actions artifacts and (optionally) S3 keys\n2. Empty sections display **filters + search scope**\n3. Configurable in `config/defaults.yml`:\n\n```yaml\nhistorian:\n  sources:\n    in_progress:\n      project_v2:\n        enabled: true\n        project_name: \"Release Copilot\"\n        status_field: \"Status\"\n        status_values: [\"In Progress\"]\n      labels: [\"in-progress\"]  # optional fallback\n\n    backlog:\n      project_v2:\n        enabled: true\n        project_name: \"Release Copilot\"\n        status_field: \"Status\"\n        status_values: [\"Backlog\"]\n\n    notes:\n      scan_issue_comments: true\n      scan_pr_comments: true\n      comment_markers: [\"Decision:\", \"Note:\", \"Blocker:\", \"Action:\"]\n      annotate_group: true\n      group_subsections: false\n\n    artifacts:\n      github_actions:\n        enabled: true\n        workflows: [\"ci.yml\",\"weekly-history.yml\"]\n      s3:\n        enabled: false\n        bucket: \"releasecopilot-artifacts\"\n        prefixes: [\"reports/\", \"history/\"]\n```\n\n4. Unit tests cover all collectors with fixtures\n5. CI lint/tests pass\n\n---\n\n## Implementation Plan (compact)\n\n1. **GraphQL helpers**  \n   - Add `github/projects_v2.py` with `query_issues_with_status(owner, repo, project_name, status_values)`\n   - Extract Status via `ProjectV2ItemFieldSingleSelectValue`\n   - Fallback to REST if project not found\n\n2. **Notes & Decisions collector**  \n   - Parse issue/PR comments updated in window  \n   - Capture lines with markers, annotate with parent group (Completed, In Progress, Backlog)  \n\n3. **Artifacts collector**  \n   - GitHub Actions: list workflow runs in window → artifacts  \n   - S3 (optional): `boto3 list_objects_v2` with prefixes + LastModified filter  \n\n4. **Renderer**  \n   - Update Markdown builder to include counts, filters, and annotated entries  \n\n5. **Config & Docs**  \n   - Add config keys in `config/defaults.yml`  \n   - Update `docs/runbooks/history-pr.md` to explain markers, filters, and how to adapt if project moves repos or statuses expand  \n\n6. **Tests**  \n   - Fixtures: `tests/fixtures/github_issues_projects.json`, `tests/fixtures/gha_artifacts.json`  \n   - Unit tests for In Progress, Backlog, Notes parsing, Artifacts  \n\n---\n\n## Tiny PR Plan\n\n- **Branch:** `feat/historian-collectors-v1`\n- **Commit msg:** `feat(historian): add in-progress/backlog/notes/artifacts collectors + template`\n- **Labels:** `enhancement`, `infra`, `codex`, `high-priority`\n\n**Checklist:**\n- [ ] Add GraphQL helpers  \n- [ ] Implement collectors (In Progress, Backlog, Notes, Artifacts)  \n- [ ] Optional S3 support (default off)  \n- [ ] Update template renderer with counts + filter info  \n- [ ] Add config keys  \n- [ ] Add fixtures & tests  \n- [ ] Update docs  \n\n**Definition of Done:** CI green; snapshot shows new sections with real data or explicit filters.\n\n---\n\n## Quick Rollback\n- Revert the PR  \n- Remove new config keys (script ignores missing ones gracefully)\n\nNote: This story introduces the initial implementation of Notes & Decisions markers. All future GitHub Issues should include at least one marker comment (Decision:, Note:, Blocker:, or Action:) so the Historian can capture context automatically.\n\n","number":114,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Add “In Progress / Upcoming / Notes / Artifacts” Collectors to Git Historian","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/114"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfNxbw","linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/129"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"Add “In Progress / Upcoming / Notes / Artifacts” Collectors to Git Historian"},{"content":{"body":"## Summary\n\nRunning the Weekly/Manual **Git Historian** with the current command line causes an immediate failure:\n\n```\npython -m scripts.generate_history --since 7d --until now --output docs/history --debug-scan\n# → error: unrecognized arguments: --debug-scan\n# exit code 2\n```\n\nThis breaks the scheduled historian and manual runs on the **develop** branch.\n\n---\n\n## Why this matters (Concepts)\n\n- The historian CLI moved to **package-style invocation** (`python -m scripts.generate_history`) and now relies on **argparse** options declared in `scripts/generate_history.py`.\n- The workflow (and our runbook) still passes a **nonexistent flag** (`--debug-scan`), which argparse correctly rejects.  \n- Result: the job exits before producing the weekly snapshot (missing docs/history updates, no PR from the bot).\n\n---\n\n## Evidence / Logs\n\n```\nBASE_CMD=(python -m scripts.generate_history --since \"$SINCE_VALUE\" --until now --output docs/history --debug-scan)\n...\ngenerate_history.py: error: unrecognized arguments: --debug-scan\nError: Process completed with exit code 2.\n```\n\n**Environment (from runner):**\n- Python: 3.10.18\n- `PYTHONPATH`: /home/runner/work/ReleaseCopilot-AI/ReleaseCopilot-AI\n- Inputs: `SINCE_INPUT=7d`, `TEMPLATE_INPUT=` (empty)\n\n---\n\n## Scope & Impact\n\n- **Affected:** Weekly historian workflow, any manual “Generate History” run that includes `--debug-scan`.\n- **Outputs missing:** `docs/history/*` snapshot files and the auto PR.\n- **Risk:** Low code change; main risk is **doc/workflow drift**.\n\n---\n\n## Root-Cause Hypothesis\n\n`--debug-scan` was added in workflow/scripts but never implemented in the CLI parser. Argparse treats unknown flags as errors. The runbook also suggests flags that are no longer valid.\n\n---\n\n## Options (Proposed Fix)\n\n**Option A — Implement `--debug-scan` in CLI (backward-compatible)**  \n- Add `--debug-scan` to `scripts/generate_history.py` as `action=\"store_true\"`.\n- When set, increase verbosity and emit extra scan details (same effect as `--log-level DEBUG` + any additional diagnostics).\n- Keep workflows/docs unchanged (they continue to pass `--debug-scan`).\n\n**Option B — Remove flag from workflows/docs**  \n- Delete `--debug-scan` from GitHub Actions and docs.\n- Rely on `--log-level DEBUG` for verbosity.\n\n**Decision:** _Implement Option A_ for backward compatibility, then **document** `--debug-scan` as a friendly alias for power users.  \n  \n**Decision:** Also update the runbook to recommend **`python -m scripts.generate_history`** with explicit `PYTHONPATH` when running locally.\n\n---\n\n## Acceptance Criteria\n\n- [ ] Historian runs succeed on manual and scheduled jobs (no CLI error).  \n- [ ] New snapshot files appear under `docs/history/` for the expected time window.  \n- [ ] An auto PR is opened by the bot with the snapshot (if configured).  \n- [ ] `--debug-scan` is documented in `--help` and the runbook.  \n- [ ] Unit test covers CLI parsing for `--debug-scan`.  \n- [ ] Runbook examples show **package invocation** and correct flags.\n\n---\n\n## Tasks (Checklist)\n\n1. **CLI:** add flag support\n   - [ ] `scripts/generate_history.py`: add `parser.add_argument(\"--debug-scan\", action=\"store_true\", help=\"Emit extra scan diagnostics\")`\n   - [ ] Wire to logging: if set → `log_level = \"DEBUG\"` (unless user overrode `--log-level`)\n   - [ ] Optionally gate extra diagnostics (e.g., file counts, skipped refs)\n\n2. **Workflow(s):** keep but harden\n   - [ ] `.github/workflows/weekly-historian.yml`: keep `--debug-scan`, ensure package invocation (`python -m ...`), and `PYTHONPATH` is set\n   - [ ] Ensure `--until now` is accepted (if CLI expects ISO; if not, switch to UTC timestamp)\n\n3. **Docs:** correct invocation & flags\n   - [ ] `docs/history/RUNBOOK.md`: replace `python scripts/generate_history.py` with `python -m scripts.generate_history`\n   - [ ] Add a “Troubleshooting” section for argparse errors\n   - [ ] Document `--debug-scan` and `--log-level`\n\n4. **Tests:** add CLI test\n   - [ ] `tests/test_generate_history_cli.py`: verify `--debug-scan` parses and toggles debug\n   - [ ] Add a smoke test with `--since 1d --until now --dry-run`\n\n5. **PR validation:** ensure artifact exists\n   - [ ] CI step fails if produced snapshot directory is empty\n   - [ ] Bot opens PR with message template\n\n---\n\n## Repro Steps\n\n1. Open the **Generate History** workflow dispatch and run against `develop` with defaults.  \n2. Observe failure: `unrecognized arguments: --debug-scan` and exit code 2.  \n3. After fix, repeat; confirm snapshot files + bot PR creation.\n\n---\n\n## PR Plan (Tiny)\n\n- Commit 1: add CLI arg + tests (`--debug-scan`)  \n- Commit 2: docs/runbook updates (package invocation, flags)  \n- Commit 3: workflow sanity (ensure PYTHONPATH; assert non-empty artifacts)  \n\n---\n\n## Codex Prompt (for quick implementation)\n\n> Implement backward-compatible support for `--debug-scan` in the Git Historian CLI.\n>\n> - File: `scripts/generate_history.py`\n> - Add `--debug-scan` (store_true). When present, default `log_level` to DEBUG unless `--log-level` is explicitly provided.\n> - Emit additional diagnostics when enabled (e.g., scanned PR count, issues count, unmatched refs, template path, output dir).\n> - Ensure `python -m scripts.generate_history` works from repo root with `PYTHONPATH` set by workflow.\n> - Update `--help` text.\n> - Add `tests/test_generate_history_cli.py` to validate parsing and behavior.\n> - Confirm the weekly historian workflow (`.github/workflows/weekly-historian.yml`) continues to pass `--debug-scan` and sets `PYTHONPATH` to repo root.\n> - Update `docs/history/RUNBOOK.md` to show package invocation + new flag.\n\n---\n\n## Learning Callout\n\n- Note: **Argparse is strict**: unknown flags stop execution. Keeping **workflows and docs aligned** with the CLI is essential.  \n- Prefer **backward-compatible** changes (add flags rather than remove) to avoid breaking scheduled jobs.  \n- Add **CLI unit tests** to catch future drift early.\n\n---\n\n## Maintenance Markers\n\nDecision: Implement `--debug-scan` in CLI; keep workflow flag for compatibility; update runbook for `python -m` package invocation.  \nNote: Ensure `--until now` is acceptable; if not, convert to an ISO timestamp at the call site.  \nAction: (Owner: Shayne) Merge fix and close by **2025-09-30**.  \nBlocker: None.\n\n---\n\n## References\n\n- Failing command from runner logs (develop)\n- Prior change to package-style invocation in notes-mirror workflow\n","number":131,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Fix: Git Historian fails with `--debug-scan` (unrecognized argument)","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/131"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfOOMM","labels":["bug","documentation","codex","cli"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"Fix: Git Historian fails with `--debug-scan` (unrecognized argument)"},{"content":{"body":"## Summary\nWe successfully stood up the foundational AWS resources for ReleaseCopilot manually in the Console. This issue tracks:\n1. Documenting the manual steps as a baseline.\n2. Migrating those resources into CDK with least-privilege policies and reproducible IaC.\n\n---\n\n## Manual Setup Completed\n1.  **IAM Role for Lambda (`ReleaseCopilotLambdaRole`)**\n    * **Temporary permissions:**\n        * S3: `AmazonS3FullAccess`\n        * Secrets Manager: `SecretsManagerReadWrite`\n        * CloudWatch Logs: `CloudWatchLogsFullAccess`\n    * **Plan:** Replace with scoped policies in CDK.\n\n2.  **S3 Bucket (`releasecopilot-artifacts-slv`)**\n    * **Region:** `us-west-2`\n    * **Configuration:** Public access blocked, versioning enabled, SSE-S3 encryption.\n    * **Lifecycle rules:**\n        * `raw/`: IA @ 30d, expire @ 90d\n        * `reports/`: IA @ 60d, no expiry\n\n3.  **Secrets Manager**\n    * **/releasecopilot/jira**: Jira OAuth tokens + email\n    * **/releasecopilot/bitbucket**: Bitbucket app password\n    * **Purpose:** Move sensitive values from `.env` → secure vault.\n\n---\n\n## Why this matters\n-   Manual setup validated the architecture end-to-end.\n-   Next step is encoding these resources in CDK for repeatability and applying least-privilege IAM.\n-   Capturing this in the repo ensures all contributors understand the project’s AWS baseline.\n\n---\n\n## Scope\n-   Add `docs/aws-setup.md` describing the manual setup (with console screenshots if possible).\n-   Create CDK constructs for:\n    -   S3 bucket (with encryption, lifecycle).\n    -   Secrets Manager (import existing ARNs or create placeholders).\n    -   IAM role with least-privilege policies for S3, Secrets, and Logs.\n-   Update Lambda definition in CDK to use this role and bucket.\n-   Replace temporary `FullAccess` managed policies with JSON policies scoped to:\n    -   `s3:PutObject/GetObject` → `<bucket>/releasecopilot/*`\n    -   `s3:ListBucket` → `<bucket>`\n    -   `secretsmanager:GetSecretValue` → secret ARNs\n    -   CloudWatch logs permissions.\n\n---\n\n## Acceptance Criteria\n-   `cdk synth` shows bucket, secrets, and IAM with least-privilege.\n-   Lambda deployed via CDK uses the new role.\n-   Documentation (`docs/aws-setup.md`) exists in repo, covering the manual baseline.\n-   Manual + CDK configs are aligned.\n\n---\n\n## Codex Prompt\nYou are codifying an existing manual AWS setup into CDK (Python). Deliverables:\n\n-   Create `infra/cdk/core_stack.py` with:\n    -   S3 bucket (SSE-S3, block public, versioning, lifecycle).\n    -   Secrets Manager: import existing ARNs via context OR create placeholder secrets.\n    -   IAM role with least-privilege permissions:\n        -   S3: `Put`/`Get` objects under `releasecopilot/*` prefix, `ListBucket` on the bucket.\n        -   SecretsManager: `GetSecretValue` on two specific ARNs.\n        -   CloudWatch Logs: `CreateLogGroup`, `CreateLogStream`, `PutLogEvents`.\n    -   Lambda function using this role, env vars set: `RC_S3_BUCKET`, `RC_S3_PREFIX`, `RC_USE_AWS_SECRETS_MANAGER`.\n-   Add `infra/cdk/app.py` to wire stack with context (env, region, bucketName, secretArns).\n-   Add `tests/infra/test_core_stack.py` with assertions for:\n    -   Bucket encryption/versioning/lifecycle.\n    -   IAM policy resources.\n    -   Lambda env vars.\n-   Add `docs/aws-setup.md` summarizing the manual steps already performed, including IAM, S3, and Secrets.\n\n**Constraints:**\n-   Use explicit ARNs for secrets.\n-   Scope IAM to least-privilege.\n-   **Outputs:** bucket name + Lambda ARN.\n","number":52,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"[AWS/CDK] Document manual AWS setup & migrate to least-privilege CDK policies","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/52"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfE43A","labels":["documentation","aws","infra","security","cdk"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"[AWS/CDK] Document manual AWS setup & migrate to least-privilege CDK policies"},{"content":{"body":"## Summary\nCreate a deployable AWS CDK (Python) skeleton that provisions:\n- **S3 artifacts bucket** (SSE-S3, block public, versioning, lifecycle for `raw/` + `reports/`)\n- **Secrets Manager** entries for Jira + Bitbucket (import existing or create placeholders)\n- **IAM execution role** (least-privilege) for a single **Lambda** function\n- **Lambda** wired with environment variables and ready to run our packaged artifact\n\nThis gives us a reproducible path to stand up Release Copilot infra in any AWS account with `cdk bootstrap && cdk deploy`.\n\n## Why this matters\nWe validated the pieces manually; moving to CDK makes infra changes versioned, reviewable, and repeatable across dev/stage/prod. It also unblocks CI/CD steps that run `cdk synth` to catch drift early.\n\n## Scope\n**New files**\n- `infra/cdk/app.py` — CDK app entrypoint (reads context/env, instantiates stack)\n- `infra/cdk/core_stack.py` — main stack: S3, Secrets, IAM role, Lambda\n- `infra/cdk/requirements.txt` — CDK libs (aws-cdk-lib, constructs)\n- `cdk.json` — optional defaults (context: env name, bucket base, schedule flags)\n- `tests/infra/test_core_stack.py` — basic assertions (encryption, lifecycle, IAM actions)\n\n**Stack details**\n- **S3 Bucket**  \n  - SSE-S3, block public, versioned  \n  - Lifecycle: `raw/` → IA @ 30d, expire @ 90d; `reports/` → IA @ 60d (no expiry)\n- **Secrets**  \n  - Import existing by ARN *or* create placeholders:  \n    `/releasecopilot/jira`, `/releasecopilot/bitbucket`\n- **IAM Role**  \n  - Trust: Lambda  \n  - Policies (least-privilege):  \n    - S3: `PutObject/GetObject` → `arn:aws:s3:::<bucket>/releasecopilot/*`  \n      `ListBucket` → `arn:aws:s3:::<bucket>`  \n    - Secrets Manager: `GetSecretValue` → specific secret ARNs  \n    - Logs: `CreateLogGroup/CreateLogStream/PutLogEvents`\n- **Lambda**  \n  - Python 3.11, handler configurable (default `main.handler`)  \n  - Env: `RC_S3_BUCKET`, `RC_S3_PREFIX`, `RC_USE_AWS_SECRETS_MANAGER=true`  \n  - Timeout 2–5 min, memory 512–1024MB (configurable)\n\n**Config & context**\n- Context keys in `cdk.json` or `--context KEY=VALUE`:  \n  - `env=dev`, `bucketBase=releasecopilot-artifacts`, `region=us-west-2`  \n  - `jiraSecretArn=...`, `bitbucketSecretArn=...` (or signal to create placeholders)  \n  - `scheduleEnabled=false`, `scheduleCron=\"cron(30 1 * * ? *)\"` (optional for later)\n\n## Acceptance Criteria\n- `cdk synth` succeeds locally; template shows:\n  - S3 bucket with SSE-S3, versioning, lifecycle rules\n  - IAM role with scoped S3 prefix, exact Secrets ARNs, and CloudWatch Logs\n  - Lambda with the role attached and env vars set\n- `pytest -q` passes for `tests/infra/test_core_stack.py` assertions:\n  - Bucket encryption & lifecycle present\n  - IAM statements contain only expected actions/resources\n  - Lambda has expected env vars\n- README section **“Deploying with CDK (dev)”**:\n  - `python -m venv .venv && source .venv/bin/activate` (Windows variant too)\n  - `pip install -r infra/cdk/requirements.txt`\n  - `cd infra/cdk && cdk bootstrap && cdk deploy --context env=dev`\n- (Optional) If context includes secret ARNs, stack **imports** them; else it **creates placeholders**.\n\n## Out of scope\n- Packaging helper changes  \n- EventBridge scheduler (separate issue already tracked)\n\n## Risks & Mitigations\n- **Over-permissioning:** Use prefix-scoped ARNs for S3 and exact ARNs for Secrets.  \n- **Region mismatches:** Read region from context; document requirement that Lambda & S3 are co-located.  \n- **Snowflake config:** Tests assert key settings to prevent silent drift.\n\n---\n## Notes for code reviewers\nKeep IAM statements minimal and explicit. Fail PRs that add wildcards or broaden resource scope without justification.\n\nCodex Prompt:\nYou are implementing a deployable AWS CDK (Python) skeleton for ReleaseCopilot-AI.\n\n### Deliverables\n1) Create `infra/cdk/app.py` that:\n   - Reads context (env, region, bucketBase, jiraSecretArn, bitbucketSecretArn, scheduleEnabled, scheduleCron).\n   - Instantiates `CoreStack` once, passing in resolved values.\n   - Uses `cdk.Environment(account=os.getenv(\"CDK_DEFAULT_ACCOUNT\"), region=context[\"region\"])`.\n\n2) Create `infra/cdk/core_stack.py` that defines `CoreStack(Construct)` with:\n   - S3 bucket:\n     - SSE-S3 (`BucketEncryption.S3_MANAGED`), `block_public_access=BLOCK_ALL`, `versioned=True`\n     - Lifecycle: `raw/` → IA after 30 days, expire after 90; `reports/` → IA after 60 days\n   - Secrets:\n     - If ARNs provided via context: import with `Secret.from_secret_complete_arn`\n     - Else: create placeholders with `Secret` and `SecretStringGenerator`\n   - IAM role for Lambda:\n     - Trust policy: `lambda.amazonaws.com`\n     - Inline policies:\n       - S3: `s3:PutObject`, `s3:GetObject` on `arn:aws:s3:::<bucket>/releasecopilot/*`\n       - S3: `s3:ListBucket` on `arn:aws:s3:::<bucket>`\n       - Secrets Manager: `secretsmanager:GetSecretValue` on provided/created secret ARNs\n       - CloudWatch Logs: `logs:CreateLogGroup`, `logs:CreateLogStream`, `logs:PutLogEvents`\n   - Lambda function:\n     - Runtime Python 3.11; handler default `main.handler` (make this configurable via context)\n     - Code: `Code.from_asset(\"dist\")` or placeholder path; accept override via context `lambdaAssetPath`\n     - Timeout 180–300s; memory 512–1024\n     - Env vars: `RC_S3_BUCKET=<bucketName>`, `RC_S3_PREFIX=\"releasecopilot\"`, `RC_USE_AWS_SECRETS_MANAGER=\"true\"`\n   - (Optional comments) where EventBridge could be wired later using context flags.\n   - Stack outputs: `ArtifactsBucketName`, `LambdaName`, `LambdaArn`\n\n3) Add `infra/cdk/requirements.txt` with:\n   - `aws-cdk-lib==2.*`\n   - `constructs>=10.0.0,<11.0.0`\n   - (dev) `pytest`, `pytest-cov`\n\n4) Add `cdk.json` example with context defaults:\n   ```json\n   {\n     \"context\": {\n       \"env\": \"dev\",\n       \"region\": \"us-west-2\",\n       \"bucketBase\": \"releasecopilot-artifacts\",\n       \"lambdaAssetPath\": \"dist\",\n       \"jiraSecretArn\": \"\",\n       \"bitbucketSecretArn\": \"\",\n       \"scheduleEnabled\": false,\n       \"scheduleCron\": \"cron(30 1 * * ? *)\"\n     }\n   }\n\nCreate tests/infra/test_core_stack.py:\n\nUse aws_cdk.assertions.Template to assert:\n\nBucket has BucketEncryption and lifecycle rules with raw/ and reports/\n\nIAM Policy document includes only expected actions/resources\n\nLambda has env vars RC_S3_BUCKET, RC_S3_PREFIX, RC_USE_AWS_SECRETS_MANAGER\n\nREADME snippet (append):\n\n“Deploying with CDK (dev)” with cdk bootstrap + cdk deploy steps for Windows & Unix shells.\n\nNote: provide --context overrides or edit cdk.json.\n\nConstraints\n\nKeep IAM least-privilege (no * actions; narrow resources to bucket prefix + specific secret ARNs).\n\nPrefer explicit props; avoid defaults that could vary by region/account.\n\nKeep file paths OS-agnostic where possible.\n\nAcceptance\n\ncd infra/cdk && cdk synth works.\n\npytest -q passes with the assertions on the synthesized template.\n\nThe stack outputs include artifact bucket and Lambda identifiers.","number":51,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"[CDK] Deployable stack skeleton: S3 + Secrets + IAM + Lambda (app.py + core_stack.py)","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/51"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfCcLw","labels":["documentation","export","aws","infra","security","cdk"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"[CDK] Deployable stack skeleton: S3 + Secrets + IAM + Lambda (app.py + core_stack.py)"},{"content":{"body":"## Summary\n\nThe GitHub Actions step is failing because `scripts/generate_history.py` does **not** support an `--until` argument. Current run:\n\n```\npython -m scripts.generate_history --since 10 --until now\n-> error: unrecognized arguments: --until now\n```\n\nThis breaks the `mirror-notes` job and prevents history from being collected/mirrored.\n\n---\n\n## Why this matters (Concepts)\n\n- **Deterministic time windows:** Our historian needs a stable window so “last N days” snapshots are reproducible in CI.\n- **Interface drift:** The workflow assumed `--until` existed; the CLI only supports `--since` today. We should either (a) add `--until` to the CLI, or (b) remove it from the workflow and rely on the CLI default (`now`).\n\n---\n\n## Acceptance Criteria\n\n1. The `mirror-notes` job completes without error on a test PR.  \n2. `scripts/generate_history.py -h` shows `--until` as an optional parameter **or** the workflow no longer passes `--until` (choose one approach below, but document which we shipped).  \n3. Unit tests cover argument parsing for time window(s).  \n4. README/docs are updated to reflect the supported flags and examples.  \n5. Historian output is generated and uploaded as usual (no behavior regressions).\n\n---\n\n## Reproduction\n\n- On `main`, run the workflow (or locally simulate the step):  \n  ```bash\n  PYTHONPATH=. python -m scripts.generate_history --since 10 --until now\n  ```\n- Observe argparse error: `unrecognized arguments: --until now`\n\n---\n\n## Proposed Fix (Two-step, safe path)\n\n### Step 1 — Hotfix workflow (fast unblock)\n- **Change** the GitHub Actions step to drop `--until now` since `now` is already the default cutoff.\n- New command:\n  ```bash\n  python -m scripts.generate_history --since 10\n  ```\n- This unblocks CI immediately while we decide on CLI surface area.\n\n### Step 2 — (Optional) Add `--until` to the CLI for flexibility\n- Extend `scripts/generate_history.py`:\n  - Add `--until` (string), default `None` → treated as “now” if not provided.\n  - Accept values:\n    - Literal: `now`\n    - ISO8601 date/time: `2025-09-26`, `2025-09-26T12:34:56`\n  - Parse to a `datetime` in UTC (document timezone handling).\n  - Pass both `since` and `until` to the history query layer.\n- Update help text and README with examples.\n- Add unit tests for: no `until`, `until now`, specific date.\n\n**Decision:** If we want the smallest change, we can **only** do Step 1 and close the issue. If we want parity with typical “since/until” CLIs, complete both steps in one PR.\n\n---\n\n## Implementation Notes (Practical Steps)\n\n### A. Workflow change\nFile: `.github/workflows/mirror-notes.yml`\n```yaml\n- name: run historian (collect + mirror)\n  run: |\n    python -m scripts.generate_history --since 10\n  env:\n    PYTHONPATH: ${{ github.workspace }}\n    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n```\n\n### B. CLI change (optional)\nFile: `scripts/generate_history.py` (argparse excerpt)\n```python\nimport argparse\nfrom datetime import datetime, timezone\n\ndef parse_until(val: str | None) -> datetime:\n    if val in (None, \"\", \"now\"):\n        return datetime.now(timezone.utc)\n    try:\n        # Try plain date first\n        return datetime.fromisoformat(val).astimezone(timezone.utc)\n    except Exception as exc:\n        raise argparse.ArgumentTypeError(f\"Invalid --until value: {val!r}\") from exc\n\nparser = argparse.ArgumentParser(...)\nparser.add_argument(\"--since\", help=\"Relative window or date (e.g., '10d' or '2025-09-01')\")\nparser.add_argument(\"--until\", type=parse_until, default=None, help=\"ISO date/time or 'now' (default)\")\n# ...\nargs = parser.parse_args()\nuntil_dt = parse_until(args.until)\n```\n\nEnsure downstream code accepts/uses `until_dt` (fallback to `now` if not provided).\n\n### C. Tests\n- `tests/test_generate_history_args.py`\n  - `--since 10` (no `--until`)\n  - `--since 10 --until now`\n  - `--since 10 --until 2025-09-26`\n\n### D. Docs\n- Update `docs/history/HISTORY_TEMPLATE.md` or `README.md` usage examples:\n  - Without `--until` (default now)\n  - With `--until now`\n  - With a concrete date\n\n---\n\n## Risk & Rollback\n\n- **Risk:** Mis-parsed dates leading to empty or oversized windows.  \n- **Mitigation:** Strict parsing + unit tests + CI dry-run step.  \n- **Rollback:** Revert workflow to omit `--until` and release a patch.\n\n---\n\n## PR Plan (tiny)\n\n1. Commit 1: drop `--until now` in workflow.  \n2. (Optional) Commit 2: add `--until` to CLI + tests + docs.  \n3. Verify CI passes and historian artifacts upload correctly.\n\n---\n\n## Codex Prompt (for implementation)\n\n> You are a Senior Python Engineer working on Release Copilot’s Git Historian.  \n> **Goal:** Fix the CI failure caused by `--until now`. Unblock quickly by updating the workflow, and optionally add `--until` to the CLI.  \n> **Tasks:**\n> 1. Edit `.github/workflows/mirror-notes.yml` to remove `--until now` from the historian step. Keep `--since 10`.  \n> 2. (Optional) In `scripts/generate_history.py`, add an argparse option `--until` that accepts `now` or ISO date/time. Parse to UTC `datetime`. If omitted, default to `now`. Thread this value to the history query logic.  \n> 3. Add unit tests covering the new argument and existing behavior.  \n> 4. Update README/docs with examples.  \n> **Done when:** CI passes, tests are green, and the historian job produces artifacts as before.\n\n---\n\n## Context Maintenance Markers\n\n- **Decision:** Treat `now` as the implicit default `until`; hotfix by removing `--until` from the workflow. Optionally add `--until` to the CLI for parity.  \n- **Note:** The previous version of the CLI never supported `--until`; the workflow assumed it did.  \n- **Action:** (Owner: Shayne) Merge PR that removes `--until` to unblock. (Owner: TBD) Add `--until` support + tests in a follow-up or same PR.  \n- **Blocker:** Workflow fails on every run until this is fixed (`exit code 2`).","number":130,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Fix: Git Historian workflow fails on `--until now` (unrecognized argument)","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/130"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfN7a0","labels":["bug","documentation","cli"],"linked pull requests":["https://github.com/dutchsloot84/ReleaseCopilot-AI/pull/129"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"Fix: Git Historian workflow fails on `--until now` (unrecognized argument)"},{"content":{"body":"Expand test coverage to ensure stability of the pipeline. Add unit tests for Jira/Bitbucket clients, config parsing, and export functions. Add integration test with sample config + dummy Jira/Bitbucket data to validate end-to-end flow.\n\nAcceptance Criteria:\nUnit tests for each module (clients/, export/, matcher/)\nMock Jira/Bitbucket API responses for repeatable tests\nIntegration test runs end-to-end with sample config\nCI pipeline fails if coverage < threshold (e.g., 70%)","number":17,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Unit + integration test suite","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/17"},"id":"PVTI_lAHOAGJYZc4BDSLLzge0NFM","labels":["enhancement","infra","testing"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"Unit + integration test suite"},{"content":{"body":"Add logic to upload the final audit outputs (Excel + JSON) to an S3 bucket.\n\nRequirements:\n\n- Use boto3 to upload\n- S3 path should include fixVersion prefix, e.g.:\n    `s3://releasecopilot-audits/fix-2025.09.27/report.xlsx`\n- Optionally take --upload flag and --bucket-name\n- Log uploaded URLs to console or file\n\nCodex Prompt:\nUsing boto3, write a function to upload `report.xlsx` and `audit.json` to an S3 bucket.\nInputs:\n- bucket_name\n- fix_version (used as a folder prefix)\n- local file paths\n\nOutput:\n- Final S3 URL per file\nAdd CLI flags: --upload, --bucket-name, --fix-version\n","number":26,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"[AWS] Upload audit artifacts to S3 with version prefix","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/26"},"id":"PVTI_lAHOAGJYZc4BDSLLzge3EeM","repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"[AWS] Upload audit artifacts to S3 with version prefix"},{"content":{"body":"The current audit command relies on a direct API call to Jira to retrieve issues via JQL. We've experienced intermittent failures with this endpoint due to known Atlassian bugs (e.g., JRACLOUD-94876), which can cause the entire audit process to fail.\n\nTo improve the tool's resilience and ensure audits can always be completed, we need to implement a fallback mechanism. If the initial Jira JQL query fails after all retries, the system should prompt the user for a path to a manually exported Jira issues CSV file. The process should then continue by loading the issues from this CSV, bypassing the failed API call.\n\nThis feature is crucial for maintaining the reliability and usability of the ReleaseCopilot tool in production environments.\n\n### Acceptance Criteria\nAPI Failure Detection: The system must detect a failed Jira JQL query (e.g., a 400 Bad Request error) after exhausting its retry attempts.\n\nUser Prompt: Upon failure, the CLI must output a clear, actionable message to the user, explaining the failure and prompting for a file path to a Jira issues CSV export.\n\nCSV Loading: The system must be able to read and parse the provided CSV file, extracting the same Jira issue data that a successful API call would have returned.\n\nWorkflow Continuity: The audit process must continue from the point of failure, using the data from the CSV as if it were returned by the Jira API.\n\nError Handling: If the user provides an invalid file path or a malformed CSV, the system must display an informative error message and terminate gracefully.\n\n### Codex Prompt\nYou are a senior DevOps engineer and an expert Python developer.\n\nIssue Reference: Implement Manual CSV Fallback for Failed JQL Query\n\nYour task is to implement a robust fallback mechanism for the Jira JQL query within the audit command. The goal is to ensure the tool can complete a release audit even when the Jira API is unreliable.\n\nCore Task\nModify the Jira data retrieval logic in audit_from_config.py. The updated workflow should follow these steps:\n\nAttempt to fetch Jira issues using the standard JQL query with the existing retry mechanism.\n\nIf the query fails after all retries, log the error and transition to the fallback workflow.\n\nPrompt the user via the CLI for a local file path to a manually exported CSV file containing the Jira issues.\n\nLoad and parse the issue data from the provided CSV.\n\nContinue the rest of the audit process using the data from the CSV as the source of truth for Jira issues.\n\nImplementation Details\nPrompting: Use a clear, user-friendly prompt.\n\nError Handling: Implement robust error handling for file not found, permission errors, and invalid CSV format.\n\nCode Location: The core logic should be implemented within the audit_from_config.py file, likely within the main function or a new helper function that wraps the Jira API call.\n\nAcceptance Criteria\nThe feature will be considered complete when the following conditions are met:\n\nThe system detects a failed Jira JQL query after exhausting retries.\n\nThe user is prompted for a CSV file path via the CLI.\n\nThe system successfully loads and parses a valid Jira issues CSV file.\n\nThe audit process continues correctly using the CSV data.\n\nThe system handles invalid file paths or malformed CSVs by exiting gracefully with an informative error.","number":30,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Implement Manual CSV Fallback for Failed JQL Query","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/30"},"id":"PVTI_lAHOAGJYZc4BDSLLzge6hpo","repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Backlog","title":"Implement Manual CSV Fallback for Failed JQL Query"},{"content":{"body":"## Summary\r\nImplement a feature that pulls GitHub issues filtered by label and writes a structured JSON artifact that downstream automation (and LLM agents) can consume. This should integrate with the existing tooling conventions in `scripts/` and reuse the GitHub CLI authentication context.\r\n\r\n## Motivation\r\n- Give release automation access to label-scoped issue snapshots for reporting and changelog generation.\r\n- Provide machine-readable context files that align with the `context/` and `reports/` directories described in the project documentation.\r\n- Reduce manual effort required to answer “what’s in scope for label X” during triage.\r\n\r\n## Scope & Approach\r\n1. **Discovery**\r\n   - Review current GitHub integration utilities (e.g., `scripts/`, `clients/`) to determine the best place for the new feature.\r\n   - Confirm configuration patterns (YAML/JSON/env vars) for storing label lists and output paths.\r\n\r\n2. **Implementation**\r\n   - Add a script or module function that accepts one or more labels, fetches matching GitHub issues, and normalizes the results into a deterministic JSON schema with metadata and issue fields.\r\n   - Persist one JSON file per label under `data/issues/` (or configurable path) and ensure directories are created as needed.\r\n   - Include logging and error handling for rate limits or missing labels.\r\n\r\n3. **Testing & Validation**\r\n   - Provide unit or integration tests (mocked responses) verifying label filtering, JSON structure, and empty results handling.\r\n   - Update documentation with usage instructions.\r\n\r\n## Acceptance Criteria\r\n- Running the new command with sample labels (e.g., `--labels bug,frontend`) produces JSON files containing the defined schema and metadata.\r\n- JSON output includes `generated_at`, `repository`, `label`, and issue details (number, title, state, assignees, milestone, updated_at, html_url).\r\n- Tests covering JSON assembly and API wrapper logic are added and passing.\r\n- Documentation explains configuration, invocation, and output location.\r\n\r\n## Open Questions / Follow-ups\r\n- Should the script support pagination limits or a max-issue count?\r\n- Do we include closed issues by default?\r\n- Future enhancement: schedule via GitHub Actions once the base feature lands.\r\n","number":40,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Fetch GitHub issues by label and export structured JSON snapshot","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/40"},"id":"PVTI_lAHOAGJYZc4BDSLLzgfBnf8","labels":["feature","automation","github-integration"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Backlog","title":"Fetch GitHub issues by label and export structured JSON snapshot"},{"content":{"body":"Create a folder called prompts/ with .md or .txt files for:\n\nBitbucket client\n\n- Jira client\n- Secrets resolution\n- Streamlit UI\n- Delta tracker\n\nThese prompts can be fed to Codex/ChatGPT later to regenerate or improve functionality. Promotes AI-compatible development practices.\n\nCodex Prompt:\nFor a Python project using AI-assisted dev (Codex), create a folder `prompts/` with:\n- prompt_bitbucket_client.txt\n- prompt_jira_client.txt\n- prompt_config_loader.txt\n- prompt_streamlit_ui.txt\nEach file should include a short natural-language prompt that describes the module's purpose and how to regenerate its code using Codex.\n","number":28,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"[Documentation] Add Codex prompt files per module","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/28"},"id":"PVTI_lAHOAGJYZc4BDSLLzge3EuE","labels":["documentation","codex","infra"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Backlog","title":"[Documentation] Add Codex prompt files per module"},{"content":{"body":"Improve local development security by externalizing secrets. Use .env files (via python-dotenv) and environment variables for Jira OAuth tokens, Bitbucket credentials, and AWS keys. Ensure no secrets are hardcoded in the repo.\n\nAcceptance Criteria:\n.env file supported for local runs (example in .env.example)\nEnvironment variables override config values at runtime\n.gitignore updated to exclude .env\nSecrets documented in README.md (local usage + AWS Secrets Manager for deployment)","number":14,"repository":"dutchsloot84/ReleaseCopilot-AI","title":"Secrets handling (local)","type":"Issue","url":"https://github.com/dutchsloot84/ReleaseCopilot-AI/issues/14"},"id":"PVTI_lAHOAGJYZc4BDSLLzge0Muw","labels":["cli","infra","security"],"milestone":{"description":"","dueOn":"","title":"MVP CLI Pipeline"},"repository":"https://github.com/dutchsloot84/ReleaseCopilot-AI","status":"Done","title":"Secrets handling (local)"}],"totalCount":76}
